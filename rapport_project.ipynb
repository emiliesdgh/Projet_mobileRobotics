{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basics of Mobile Robotics project<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "**07.12.2023**\n",
    "\n",
    "**Group 23 : Diana Bejan** (325029)**, Emilie Grandjean** (286734)**, Garance Boesinger** (310447)**, Juan Martín** (376659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Introduction](#toc1_)    \n",
    "2. [General Setup](#toc2_)    \n",
    "2.1. [Project description](#toc2_1_)    \n",
    "2.2. [Environment Setup](#toc2_2_)    \n",
    "2.3. [Best Path Calculations KIKE](#toc2_3_)    \n",
    "2.4. [Motion Control](#toc2_4_)    \n",
    "2.5. [Obstacles avoidance](#toc2_5_) \n",
    "\n",
    "3. [Required Components](#toc3_)   \n",
    "3.1. [Computer Vision](#toc3_1_)    \n",
    "3.2. [Global Navigation](#toc3_2_)    \n",
    "3.3. [Motion Control](#toc3_3_)    \n",
    "3.4. [Local Navigation](#toc3_4_)    \n",
    "3.5. [Filtering](#toc3_5_)    \n",
    "\n",
    "4. [VIDEOS ? IMAGES ? LITTLE EXTRAS ?](#toc4_) \n",
    "\n",
    "5. [Overall Project](#toc5_)    \n",
    "\n",
    "6. [Conclusion](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Introduction](#toc0_)\n",
    "\n",
    "As part of the Basics of Mobile Robotic class given by M. Mondada, we are asked to implement a project using the Thymio robot. This project involves the robot reaching a goal while moving in an environment filled with permanent obstacles. During it's path, some extra physical obstacles can be placed in its way. In such moments, the Thymio has to be able to avoid them and continue its route to the end point.\n",
    "\n",
    "This is then done using several concepts seen in class such as image processing and pattern detection with Computer Vision, the Motion Control using a controller, Global and Local Navigation of the Thymio and Filtering.\n",
    "\n",
    "In this report, we will talk about the different modules implemented and detail how they work to explain our overall functionnality of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[General Setup](#toc0_)\n",
    "\n",
    "## 2.1. <a id='toc2_1_'></a>[Project description](#toc0_)\n",
    "\n",
    "1. **Create an environnment :** Our environment has to contain a set of obstacles that the Thymio avoids through *global navigation*. That is to say, the Thymio should avoid the obstacles without using the sensors to detect them.\n",
    "\n",
    "2. **Find the best path :** The obective is that the Thymio goes from an *arbitrary point* in the map to a *target* that can be placed <u>anywhere</u> in the environment.  These will be changed during the demo to see how the system performs.\n",
    "\n",
    "3. **Motion Control & Position estimation:** We will *control* the robot to help it move along the path. This requires an accurate estimate of the position of the robot which we will have to obtain through *bayesian filtering*.\n",
    "   \n",
    "4. **Avoid Obstacles :** While navigating, the Thymio will have to use *local navigation* to avoid *physical obstacals* that can be put in its path <u>at any point in time</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[Environment Setup](#toc0_)\n",
    "\n",
    "Our environment setup has no boundary points and extend to the full viewing angle of the camera, so that the full picture detected can be exploited. It includes the **goal** (red square), **static obstacles** (black, shapes with sharp lines and edges, they cannot be curvilinear), a **background** (ideally white but it would work with any light color) and the **Thymio's position** (obtained with the colored circles, blue in the front and green in the middle, placed ontop of it). The choise of these features is highly related to the vision part as the image captured needs to be processed in the Computer Vision part of our project.\n",
    "\n",
    "As for the static obstacles, we decided to have them in 2D to avoid any shadows that could compromise the detection by the camera. Further explanations on how these obstacles and features are detected is given in the Computer Vision part. \n",
    "\n",
    "An example of a frame captured by the camera with the different features mentionned is illustrated here below : \n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. <a id='toc2_3_'></a>[Best Path Calculations KIKE](#toc0_)\n",
    "\n",
    "short explication on what method used and why and how and quickly mention that it uses the info send by the CV picked up by the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. <a id='toc2_4_'></a>[Motion Control](#toc0_)\n",
    "\n",
    "For the Motion Control, we decided to only set speeds. They are chosen with respect to the distance between the current position to the next point on the path as well as the differance between the current angle and the desired angle. These values are obtained through the Kálmán Filter and the Global Navigation. Essentially, the motion is a continuous exchange between turning and advancing phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. <a id='toc2_5_'></a>[Obstacles avoidance](#toc0_)\n",
    "\n",
    "For the obstacle avoidance section, we decided to use 3D physical obstacles that cannot be detected by the camera. This will push the use of the Thymio's sensors to detect these obstacles. Since they can be put at anytime anywhere in the robot's path, using **Local Navigation** will allow to avoid them and once the obstacle avoided, the robot need to go back to following it's given path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Required Components](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def fonction()`  | input input input | output output output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Computer Vision](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The **Vision** module is mostly based on the OpenCV documentation. Other sources used are mentionned throughout the explanation. \n",
    "\n",
    "The purpose of the computer vision part, which is autonomous from the other parts but essential to compute the reference points of the setup, is to capture and use the picture from the camera to extract the main features of the environment, analyze them and give the required informations to the Global Navigation and Odometry modules (i.e. actual robot position corrdinates and angle, the goal and obstacles positions). The particularity of the Computer Vision module is that it does not communicate with the robot. \n",
    "\n",
    "The selected approach is based on color, shape and corners recognition. The first approach was with \"perfect\" images of a setup created virtually. Once switched to to the \"noisy\" pictures of the camera, we had to rescale quite a bit the values.\n",
    "\n",
    "As for the code implemented, we created a vision class containing the basic arguments computed by vision and the functions that will be called in the main when needed. Some vision informations have to be recomputed at each step, such as the robot position (coordinates and angle), and some of them only need be computed once in the first step, such as the obstacles positions and their corners that do not move during execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Image capturing and image filtering**\n",
    "\n",
    "At first, a frame is extracted from the camera. The camera has already been activated in the main (video registered in the variable **cap**). Note that we capture two frames because we noticed that sometimes, the initial connection of the camera to the computer, the first frame captured is \"yellowish\" and too bright to be used. Then, we filter the image several times to avoid noise and have a \"smoother\" picture to perform the next steps.\n",
    "\n",
    "For the image filtering, we tested several combinations and the one in the function **capture_image()** seems to work best. Therefore, we perform 2D linear filtering, then blur the frame and then apply an additional median blur. Our filtered frame is stored in the vision Class so it can used by all the other vision functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_image(self,cap): \n",
    "    #capture a frame out of the video that will be used through CV part \n",
    "    ret, self.frame = cap.read()\n",
    "    ret, self.frame = cap.read()\n",
    "    kernel = np.ones((5,5),np.float32)/25\n",
    "    img = cv2.filter2D(self.frame,-1,kernel)\n",
    "    img = cv2.blur(img,(5,5))\n",
    "    self.img = cv2.medianBlur(img,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Goal position, thymio position and angle tracking**\n",
    "\n",
    "To find the coordinates of the goal and start position, we use color filtering. We apply a color mask of the target color (red for the goal, blue and green for the robot) on the picture and then use maxLoc function or contour finding to extract the coordinates. The values of the color mask have been found using hsv color space graph and forums, they are empiric values adapted to our setup, and to the colors of the papers we are using. Indeed, one of the main \"flaws\" of the vision part, that will be discussed more in details in the weakness part, is that it is quite sensitive to light and color changes and uniformity.\n",
    "\n",
    "The function find_goal_pos applies a red mask to the filtered frame, isolating the goal shape. Then we apply a contour detection on the thresholded image and find the moment, i.e. center of the shape. This value corresponds to the goal position (center of the red square). \n",
    "\n",
    "The functions find_start_pos and find_angle provide the coordinates of the two points on top of the thymio. As the circle is of small radius, we use maxLoc function to compute the coordinates, so it is not the exact center of the point but it is precise enough (more or less 10points on a grid of 480x640). The green point is used as starting point, i.e. as the position of the thymio. The blue one is used to compute the angle of the thymio. We obtain a positive angle going from zero to two py starting from the x axis and turning counterclockwise. We made several measures for the odometry part comparing measured value and real values, and the precision of the angle is +-0.01rad. \n",
    "\n",
    "At the end of the functions computing the positions and angle, we store those values in the shared Thymio Class to be used by the other functions of the code.\n",
    "\n",
    "The *find_angle* function pasted below computes the coordinates of the blue point on the thymio at first in the same way the coordinates of the green (=start point) are computed. Then, using both green (back) and blue (front) coordinates, we compute the angle. \n",
    "\n",
    "Below is the thresholded image of the frame given as example with red mask applied, and of the green point on the robot, along with the coordinates computed by the functions. \n",
    "\n",
    "![2.png](attachment:2.png)\n",
    "\n",
    "![3.png](attachment:3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_angle(self,robot):\n",
    "    # COORDINATES OF THE BLUE POINT \n",
    "    hsv = cv2.cvtColor(self.img, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv, self.LOW_BLUE, self.HIGH_BLUE)\n",
    "    blue = cv2.bitwise_and(self.frame,self.frame, mask = mask)\n",
    "    gray = cv2.cvtColor(blue, cv2.COLOR_RGB2GRAY) \n",
    "    _, self.thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY) \n",
    "    (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(self.thresh1)\n",
    "    [x_front,y_front] = maxLoc\n",
    "    # WE VERIFY ONE BLUE POINT IS ACTUALLY DETECTED TO AVOIR HIDING SITUATIONS (if the robot is hidden, we do not\n",
    "    # recompute the coordinates)\n",
    "    if (x_front != 0) or (y_front != 0):\n",
    "        [self.x_front,self.y_front] = [x_front,y_front]\n",
    "    # ANGLE COMPUTATION\n",
    "        if self.y_front < self.y_back : \n",
    "            if self.x_front > self.x_back : \n",
    "                self.teta = np.arccos((self.x_front-self.x_back)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "            if self.x_front <= self.x_back : \n",
    "                self.teta = np.pi - np.arccos((self.x_back-self.x_front)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "        if self.y_front >= self.y_back : \n",
    "            if self.x_front > self.x_back : \n",
    "                self.teta = 2*np.pi - np.arccos((self.x_front-self.x_back)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "            if self.x_front <= self.x_back : \n",
    "                self.teta = np.pi + np.arccos((self.x_back-self.x_front)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "    # PASS THE VARIABLES TO THE SHARED THYMIO CLASS AND INDICATE THAT VISION IS DONE\n",
    "        robot.setPositions(self.x_back,self.y_back,self.x_goal,self.y_goal,self.teta)\n",
    "        robot.setVisionDone(True)\n",
    "    # If hidden thymio, do not update the vision variables \n",
    "    else:\n",
    "        robot.setVisionDone(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Obstacles**\n",
    "\n",
    "For the obstacles identification part, it depends on the type of global navigation used. As discussed in the next *Gloabl Navigation* section, we implemented two methods for global part, Astar and Visibility.  \n",
    "\n",
    "In the case of visibility graph, the process used is the following:  \n",
    "At first, we apply a big thresholding on the gray image to obtain a frame containing only the obstacles (for a noisy image, we set an empirical value of 40, which keeps only the black obstacles). The corners and moments of the obstacles are obtained using openCV corners and moments finding functions. We then implemented a small part of code placing the corners further away from the shapes to avoid the thymio running into the obstacles when following the side of a shape. The value of the distance of the corners to their shape is a variable that can be changed in the code, and that should typically be half the size of the thymio. We also took out the corners too close to the borders as we do not want the thymio to go outside the setup boundaries. Those methods are implememted in the function *find_corners*.  \n",
    "\n",
    "Then we have to provide the *global navigation* part a \"visibility graph\" with the nodes and weights of each node to all others. Obviously the nodes considered are the corners plus start and goal position. We therefore compute a matrix containing the distance of each point to all others, implemented in the *compute_dist_mx* function. But we cannot just calculate the distance between points because if there is an obstacle in the way between two points, it must not be taken into account. We therefore decided to use threshold on the line to avoid this. Here is a quick explanation of the approach: on the thresholded image of the obstacles, if we extract the pixel values of the line between two points, if it contains black, there is an obstacle in the way and we do not compute the distance. \n",
    "\n",
    "To optimally use this 'line threshold' technique, and before calling compute_dist_mx function, we call the function *trace_contours* that traces the lines beteween the corners to avoid computing path in the \"forbidden area\" between obstacles and corners. \n",
    "\n",
    "Below is a picture of the frame used for this setup to compute the distance matrix, and an example of a distance matrix obtained for the setup provided as illustration. Distance_matrix should be read as follow: indices along vertical and horizontal dimensions are [start_point, corner1, corner2, ..., cornerN, goal_point], matrix_element[i,j] = distance between corner[i] and corner[j], and if there is an obstacle in the path, distance = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corners(self):\n",
    "    # Put the filtered image in grayscale\n",
    "    gray = cv2.cvtColor(self.img, cv2.COLOR_BGR2GRAY) \n",
    "    # Threshold the image to keep only black elements\n",
    "    ret, thresh2 = cv2.threshold(gray, self.BLACK_THRESHOLD, 255, cv2.THRESH_BINARY) \n",
    "    # Find contours of obstacles \n",
    "    contours, _ = cv2.findContours( \n",
    "        thresh2, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    self.thresh2 = thresh2\n",
    "    i = 0\n",
    "    m = []\n",
    "    # Find moments, i.e. centers of obstacles \n",
    "    for contour in contours:  \n",
    "        if i == 0: \n",
    "            i = 1\n",
    "            continue\n",
    "        M = cv2.moments(contour) \n",
    "        if M['m00'] != 0.0: \n",
    "            x = int(M['m10']/M['m00']) \n",
    "            y = int(M['m01']/M['m00']) \n",
    "            if len(m)<self.NB_SHAPES:\n",
    "                m.append([x,y])\n",
    "    th3 = cv2.adaptiveThreshold(self.thresh2,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "                                cv2.THRESH_BINARY,11,2)\n",
    "    # Find corners of obstacles \n",
    "    corners = cv2.goodFeaturesToTrack(th3, self.NB_CORNERS, 0.01, 45) \n",
    "    corners = np.int0(corners) \n",
    "    # Part of code that changes the coordinates of the corners to put them away from the obstacles \n",
    "    \n",
    "    ####*** CODE TO PUT THE CORNERS AWAY FROM THE OBSTACLES ***####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5.png](attachment:5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_contours(self):\n",
    "    a = int(np.size(self.cornerss)/2)\n",
    "    b = int(np.size(self.m_cor)/2)\n",
    "    for i in range (0,a):\n",
    "        for j in range (0,b):\n",
    "            if i != j:\n",
    "                if self.m_cor[i] == self.m_cor[j]:\n",
    "                    line = np.transpose(np.array(draw.line(self.cornerss[i][0], self.cornerss[i][1], self.cornerss[j][0], self.cornerss[j][1])))\n",
    "                    data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "                    if np.size(np.where(abs(np.diff(data))>0)[0]) <= 2:\n",
    "                        if np.mean(data) > self.mean_value_along_line:\n",
    "                            cv2.line(self.thresh2, self.cornerss[i], self.cornerss[j], self.bluepx, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6.png](attachment:6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_mx(self,robot):\n",
    "    s = int(((np.size(self.cor))/2)+2) \n",
    "    dist_mx = np.zeros((s,s))\n",
    "    for i in range(1,s):\n",
    "        if i == (s-1):\n",
    "            line = np.transpose(np.array(draw.line(self.x_back,self.y_back, self.x_goal,self.y_goal)))\n",
    "            data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "            if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                dist_mx[i][0] = np.sqrt(np.power((self.x_goal-self.x_back),2)+np.power((self.y_goal-self.y_back),2))\n",
    "            continue \n",
    "        line = np.transpose(np.array(draw.line(self.x_back,self.y_back,self.cor[i-1][0],self.cor[i-1][1])))\n",
    "        data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "        if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "            dist_mx[i][0] = np.sqrt(np.power((self.cor[i-1][0]-self.x_back),2)+np.power((self.cor[i-1][1]-self.y_back),2))\n",
    "    for i in range(0,(s-2)):\n",
    "        for j in range(0,s):\n",
    "            if j == (i+1):\n",
    "                continue \n",
    "            if j == 0 : \n",
    "                line = np.transpose(np.array(draw.line(self.x_back,self.y_back, self.cor[i][0],self.cor[i][1])))\n",
    "                data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "                if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                    dist_mx[j][i+1] = np.sqrt(np.power((self.cor[i][0]-self.x_back),2)+np.power((self.cor[i][1]-self.y_back),2))\n",
    "                continue \n",
    "            if j == int(s-1):\n",
    "                line = np.transpose(np.array(draw.line(self.x_goal,self.y_goal, self.cor[i][0],self.cor[i][1])))\n",
    "                data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "                if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                    dist_mx[j][i+1] = np.sqrt(np.power((self.cor[i][0]-self.x_goal),2)+np.power((self.cor[i][1]-self.y_goal),2))\n",
    "                continue\n",
    "            line = np.transpose(np.array(draw.line(self.cor[j-1][0],self.cor[j-1][1],self.cor[i][0],self.cor[i][1])))\n",
    "            data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "            if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                dist_mx[j][i+1] = np.sqrt(np.power((self.cor[i][0]-self.cor[j-1][0]),2)+np.power((self.cor[i][1]-self.cor[j-1][1]),2))\n",
    "    for i in range(0,(s-1)):\n",
    "        if i == 0:\n",
    "            line = np.transpose(np.array(draw.line(self.x_back,self.y_back, self.x_goal,self.y_goal)))\n",
    "            data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "            if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                dist_mx[0][s-1] = np.sqrt(np.power((self.x_goal-self.x_back),2)+np.power((self.y_goal-self.y_back),2))\n",
    "            continue \n",
    "        line = np.transpose(np.array(draw.line(self.x_goal,self.y_goal,self.cor[i-1][0],self.cor[i-1][1])))\n",
    "        data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "        if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "            dist_mx[i][s-1] = np.sqrt(np.power((self.cor[i-1][0]-self.x_goal),2)+np.power((self.cor[i-1][1]-self.y_goal),2))\n",
    "    self.dist_mx = dist_mx\n",
    "    robot.setDistMx(dist_mx)\n",
    "    robot.setCor(self.cor)\n",
    "    robot.setS(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a distance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the vision work for the Astar algorithm, we provide the global navigation part an occupancy matrix, containing 0 where the space is free and 1 where there is an obstacle. We first need to resize the image because Astar needs a smaller grid to operate quickly enough. We can set the resizing parameters in the code.  \n",
    "\n",
    "We use the obstacle image and transform it into a matrix, and then apply a threshold on the values of the matrix to obtain a binary 0/1 matrix, and store the occupancy matrix in the shared robot Class instance. This is implemented in the function **return_occupancy_matrix()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_occupancy_matrix(self,robot):\n",
    "    dim = (self.width_resized, self.height_resized)\n",
    "    img = cv2.resize(self.img, dim, interpolation = cv2.INTER_AREA)\n",
    "    rszd = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mtx = np.array(rszd)\n",
    "    mx = (mtx < 20).astype(int)\n",
    "    robot.occupancy_matrix = mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of the flaws of the computer vision part**\n",
    "\n",
    "- The color filtering is fun and useful but with hsv space and masking, we explore a \"range\" of values (for example \"reddish\" values). In noisy pictures, if the range is too large, we will detect a little bit of this color somewhere it should not be, and on the contrary if the range is too small we will not detect the target, or we will detect it in one lightning condition and not the other. To have an optimal color detection, we should have \"perfect\" pictures which, in reality, do not exist. It is therefore an issue to be dealt with. In our case, once the picture was well filtered and the mask were adapted to the colors we used, it worked well in most situations. \n",
    "\n",
    "- The functions implementing the distance matrix, putting away corners and thresholding along lines have been computed fully \"by hand\" and are surely not optimized. Using existing libraries and tools, we could optimize those functions and clean them out.\n",
    "\n",
    "- The visibility part is very sensitive to lightning conditions.\n",
    "\n",
    "- Our setup and code would not work with faded colors, bright light spot somewhere on the frame, artificial colored light or unidentified obstacles.\n",
    "\n",
    "- Our approach does not support round shaped obstacles.\n",
    "\n",
    "- The algorithm needs to be provided the exact number of shapes and corners before the execution. Those variables can be easily modified in the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of Vision in the main**\n",
    "\n",
    "At the beginning of the execution, a new instance of *vision Class* is created, along with a capture of the video taken by the camera. In the infinity loop, the vision functions are always called first as those values are needed for the rest of the program. It is implememted as follows:\n",
    "\n",
    "- vision Class instance initialized outside the while loop at the beginning of the execution.\n",
    "\n",
    "- initialization of the video captured from the camera.\n",
    "\n",
    "In the infinity loop:  \n",
    "- Every time, we capture a new frame, find the goal and thymio position and angle. \n",
    "\n",
    "- We only compute the contours and corners of the obstacles once.\n",
    "\n",
    "- The distance matrix for the visibility graph (+ path recomputing discussed in the global navigation part) is computed the first time we execute the loop, plus everytime we are in a kidnapping situation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb Cell 26\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y141sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m Thymio\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y141sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfiltering\u001b[39;00m \u001b[39mimport\u001b[39;00m KalmanFilter\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y141sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvision\u001b[39;00m \u001b[39mimport\u001b[39;00m Vision\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y141sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mglobal_visibility\u001b[39;00m \u001b[39mimport\u001b[39;00m Global_Nav\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y141sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m robot \u001b[39m=\u001b[39m Thymio() \u001b[39m# Set Thym as class Thymio as initialization before the while\u001b[39;00m\n",
      "File \u001b[0;32m~/Projet_mobileRobotics/Projet_mobileRobotics/vision.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m colors\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m draw\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m Thymio\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m clear_output\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "#Classes initialization\n",
    "from classes import Thymio\n",
    "from filtering import KalmanFilter\n",
    "from vision import Vision\n",
    "from global_visibility import Global_Nav\n",
    "\n",
    "robot = Thymio() # Set Thym as class Thymio as initialization before the while\n",
    "KF = KalmanFilter()\n",
    "vision = Vision()\n",
    "global_nav = Global_Nav()\n",
    "\n",
    "#Video capturing \n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "a = 0\n",
    "while(1) :\n",
    "    a = a + 1\n",
    "    #VISION\n",
    "    vision.capture_image(cap)\n",
    "    vision.find_goal_pos()\n",
    "    vision.find_start_pos(robot,a)\n",
    "    vision.find_angle(robot)\n",
    "    if (a == 1) : \n",
    "        vision.find_corners()\n",
    "        vision.trace_contours()\n",
    "    if ((robot.vision == 1) & (a == 1))or((a != 1) & (robot.kidnap == True) & (robot.vision == 1)):\n",
    "        vision.compute_dist_mx(robot)\n",
    "        global_nav.dijkstra(robot)\n",
    "        global_nav.extract_path(robot)\n",
    "        if robot.kidnap == True:\n",
    "            robot.kidnap = False\n",
    "    robot.vision = True\n",
    "    #END VISION \n",
    "    \n",
    "    ###***REST OF THE CODE***###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources :**\n",
    "\n",
    "TP2 and 'Project Computer Vision tutorial' of the *Basics of mobile robotic* course  \n",
    "OpenCV documentation: https://docs.opencv.org/4.x/  \n",
    "https://www.geeksforgeeks.org/how-to-detect-shapes-in-images-in-python-using-opencv/  \n",
    "https://www.geeksforgeeks.org/python-detect-corner-of-an-image-using-opencv/  \n",
    "https://stackoverflow.com/questions/30331944/finding-red-color-in-image-using-python-opencv  \n",
    "https://www.delftstack.com/fr/howto/python/opencv-inrange/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Global Navigation](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The **Global Navigation** module goal is to provide a path for the *motion control* module, based on the informations provided by the *vision* module. We implemented two different global navigation methods that both ended working, but do not use the same setup and variables. First, an explanation about visibility graph method will be given. We use the distance matrix between the points provided in the robot class instance and compute the best path using Dijkstra's algorithm in the function *dijkstra*, based on the pseudo-code we found here: https://www.baeldung.com/cs/dijkstra#:~:text=Dijkstra's%20Algorithm%20is%20a%20pathfinding,we%20reach%20the%20end%20node.  \n",
    "\n",
    "The output of this function is a *graph* matrix containing, for each node, its state ('visited' or not), its score to the start point and, most importantly, its nearest node (to the start).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra(self,robot):\n",
    "\n",
    "    self.dist_mx = robot.getDistMx()\n",
    "    self.s = robot.getS()\n",
    "    graph = np.zeros((3,self.s))\n",
    "    graph[0] = 3000 \n",
    "    graph[0][0] = 0\n",
    "    graph[2] = 42\n",
    "    result = 0\n",
    "    current_node = 0\n",
    "    a = 0\n",
    "    while 1:\n",
    "        b = 0\n",
    "        for i in range (0,self.s):\n",
    "            if graph[1][i] == 0: \n",
    "                b = b + 1\n",
    "                if graph[0][i] < graph[0][result]:\n",
    "                    result = i\n",
    "                if b == 1:\n",
    "                    result = i\n",
    "        current_node = result \n",
    "        graph[1][current_node] = 1\n",
    "        for i in range (0,self.s):\n",
    "            if self.dist_mx[i][current_node] != 0:\n",
    "                if graph[1][i] == 0: \n",
    "                    new_score = graph[0][current_node] + self.dist_mx[i][current_node]\n",
    "                    if new_score < graph[0][i]:\n",
    "                        graph[0][i] = new_score \n",
    "                        graph[2][i] = current_node \n",
    "        if current_node == int(self.s-1):\n",
    "            break\n",
    "        for i in range (0,self.s):\n",
    "            if graph[1][i] == 0: \n",
    "                if graph[0][i] < graph[0][result]:\n",
    "                    result = i\n",
    "                    if graph[0][result] == 3000:\n",
    "                        break\n",
    "\n",
    "    self.graph = graph \n",
    "    self.current_node = current_node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the function **extract_path()** to compute in a single array the coordinates of the path points, from start point (Thymio) to goal. This is done by extracting the coordinates using *graph* matrix, going from goal through each 'closest node' until start position. We then put this path in the shared robot class to be used by *motion control* module.  \n",
    "\n",
    "Below is an example of a path computed by the Visibility Graph algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(self,robot):\n",
    "    cor = robot.getCor()\n",
    "    path = [self.current_node]\n",
    "    node = self.current_node\n",
    "    while node != 0:\n",
    "        path.append(int(self.graph[2][node]))\n",
    "        node = path[-1]\n",
    "    path = np.flip(path)\n",
    "    path_coord = [[robot.pos_X,robot.pos_Y]]\n",
    "    a = 0\n",
    "    for p in path : \n",
    "        a = a + 1 \n",
    "        if a != 1:\n",
    "            if a != np.size(path):\n",
    "                i = int(p-1)\n",
    "                path_coord.append(cor[i])\n",
    "    path_coord.append([robot.goal_X,robot.goal_Y])\n",
    "    robot.path = path_coord\n",
    "    if len(path_coord)>1:\n",
    "        robot.setAngle()\n",
    "    self.path = path \n",
    "    self.path_coord = path_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![7.png](attachment:7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now discuss the *Global Navigation* algorithms for Astar method, using the occupancy_matrix provided by the *vision part*.  \n",
    "\n",
    "BLABLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of the differences between Astar and Visibility**\n",
    "\n",
    "- The Astar computes a more 'optimal' path than Visibility but is way slower for a big grid. The disadvantage is therefore that we have to resize the frame to a lower scale, loosing resolution, and therefore having to put only big square obstacles on defined emplacements.  \n",
    "\n",
    "- The Visibility Graph forces us to navigate between points and most of the time to follow obstacles contours. But it is way faster than Astar, even on big grids, which allows us to use almost any shape and range of size for the obstacles.  \n",
    "\n",
    "To use both methods, we therefore have to change the setup between testings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[Motion Control](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "This module of our project is used to give the wheels the necessary speeds to make the right movements. Three functions are present : the PID controller **PID_control()**, the turning function **turn()**, and the forward motion function **go_to_next_point()**. We switch between two phases while going from one point to the next. First, the turning phase where we use **turn()** to get in the angle that allows the robot to only move forward to reach the next point. Second, the advancing phase, which advences to the next point. \n",
    "\n",
    "The module is only called when there is no obstacle detected by the robot's proximity sensors. The functions work in a point-to-point and moment-to-moment fashion. This means that the angles taken into account are the current angle given by the Kálmán filter as well as the goal angle calculated between the first and second points of the path. The speeds are calculated for the distance between the two aforementioned points rather than from the current position to the final goal position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. PID Controller**\n",
    "\n",
    "The code has been inspired by the PI controller implemented in the course MICRO-315 given by Prof. Mondada. The PID controller is used for angle correction and is an integral part of both the turning and the forward motion phases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def PIDController()`  | the error between the current angle and the desired one <br> the object of class Thymio that the changes apply to| the rotational speed to give to the wheels |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, we look at the actual angle, *current_angle*, given by the filter,  and the goal angle of the robot, *robot.goal_angle* and using the difference between the two, *error*, the integral error, *robot.int_error*, the previous error, *robot.prev_error*, as well as the time difference between two calls of the PID controller to calculate an optimal speed for attaining the desired angle. \n",
    "\n",
    "**Add some explenation about how get the time difference**\n",
    "\n",
    "While in **turn()** it is the only calculation of speed, in **go_to_next_point()** it adds a rotational speed to a forward speed already calculated. The coefficients KP, KI and KD have been experimentally chosen during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**2. Turning function**\n",
    "\n",
    "The first function the program goes through between two points is the turning function, turn. This function takes as numerical inputs the current angle, given by the filter,  and the goal angle of the robot, as calculated from the path in the Thymio class. The PID controller is called for the calculation of the speed that is then passed to the robot by the functions setSpeedLeft/setSpeedRight found in the class Thymio. A helper variable, goal_reached_t is used to signal whether the turning phase has ended and the forward motion can start.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def turn()`  | the current angle <br> the object of class Thymio that the changes apply to  <br> the node which the information is sent to | none, the setting of the speeds is done internally |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Forward motion function**\n",
    "\n",
    "Once the turning phase is done, as signalled by *goal_reached_t*, the advancing phase can be accessed. This function calculates and sends speeds similar to the turning phase. The speed is proportionally controlled to slow down and smoothly reach the next point. The coefficient K has also been experimentally chosen during testing. We use the PID controller to add a rotational speed in order to correct the angle and so to make sure that the robot stays on track.\n",
    "\n",
    "Once the goal is reached, the second point in the path is discarded (the first one in the current position as given by the camera) and a new goal angle is calculated using the **setAngle()** function found in the file *classes.py* . A helper variable goal_reached_f is used to signal that the motion phase has ended and a new turning motion can begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def go_to_next_point()`  | the current angle <br> the current_position <br> if an obstacle is detected <br> the node which the information is sent to | none, only internal changes and speed setting|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmotion_control\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m Thymio\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y112sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m client \u001b[39m=\u001b[39m ClientAsync()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y112sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39mwait_for_node()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emscomputer/Projet_mobileRobotics/Projet_mobileRobotics/rapport_project.ipynb#Y112sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mawait\u001b[39;00m node\u001b[39m.\u001b[39mlock()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tdmclient/clientasync.py:42\u001b[0m, in \u001b[0;36mClientAsync.__init__\u001b[0;34m(self, node_class, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, node_class\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39msuper\u001b[39;49m(ClientAsync, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_class \u001b[39m=\u001b[39m node_class \u001b[39mor\u001b[39;00m tdmclient\u001b[39m.\u001b[39mClientAsyncCacheNode\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tdmclient/client.py:98\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, zeroconf, zeroconf_all, tdm_ws, tdm_addr, tdm_port, tdm_transport, password, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_transport \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTDM \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_addr\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_port\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend_handshake(password)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tdmclient/client.py:112\u001b[0m, in \u001b[0;36mClient.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm \u001b[39m=\u001b[39m TDMConnectionWS(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_addr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_ws_port)\n\u001b[1;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm \u001b[39m=\u001b[39m TDMConnection(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtdm_addr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtdm_port)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tdmclient/tcp.py:104\u001b[0m, in \u001b[0;36mTDMConnection.__init__\u001b[0;34m(self, host, port, debug)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m    102\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39msendall(b)\n\u001b[0;32m--> 104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mio \u001b[39m=\u001b[39m TCPClientIO(host, port)\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m=\u001b[39m debug\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tdmclient/tcp.py:96\u001b[0m, in \u001b[0;36mTDMConnection.__init__.<locals>.TCPClientIO.__init__\u001b[0;34m(self, host, port)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, host, port):\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m socket\u001b[39m.\u001b[39msocket(socket\u001b[39m.\u001b[39mAF_INET, socket\u001b[39m.\u001b[39mSOCK_STREAM)\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((host, port))\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "\"Add code to show how this this works between two points\"\n",
    "from tdmclient import ClientAsync\n",
    "import motion_control\n",
    "from classes import Thymio\n",
    "\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()\n",
    "await node.wait_for_variables()\n",
    "\n",
    "Thym=Thymio()\n",
    "Thym.path=[[0,10],[2,50]]\n",
    "Thym.pos_X=Thym.path[0][0]\n",
    "Thym.pos_Y=Thym.path[0][1]\n",
    "Thym.setAngle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Thym.goal_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Management of the reaching of the final goal position**\n",
    "\n",
    "When the length of the path is down to one, the goal position has been reached and the work is done. We see this in the code for **go_to_next_point()**, where there is a condition that skips the function if the path length is too short. The speeds are set to 0 as a advancing phase has been completed, and no new angle is calculated nor is the path changed anymore, as only the current position is left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_to_next_point(current_angle, current_position, obstacle, robot, node): \n",
    "    if len(robot.path) > 1:\n",
    "        '''code...'''\n",
    "    else:   # Advancing phase in completed when close enough to the desired point\n",
    "        robot.goal_reached_f = True\n",
    "        robot.goal_reached_t = False\n",
    "        robot.prev_error = 0\n",
    "        robot.int_error = 0\n",
    "        robot.prev_time = 0\n",
    "        robot.setSpeedRight(0,node)\n",
    "        robot.setSpeedLeft(0,node)\n",
    "        robot.path.pop(1)   # Path shortened\n",
    "\n",
    "    if len(robot.path) > 1:\n",
    "        robot.setAngle()    # Angle changed with respect to the new goal\n",
    "                \n",
    "    else:\n",
    "        pass    # Function skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the code above, the condition has to be introduced in the advancing phase completion as the* *robot.setAngle()** function takes the shortened path as an input before it is taken into account by the whole function in a future call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[Local Navigation](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The **Local Navigation** module is inspired by the exercise session *Week 3 - Artificial Neural Networks* paragraph 5. The local_navigation.py file contains 3 functions, two verification functions and the actual function that makes the Thymio avoid obstacles which would be placed in its path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Testing the presence of an obstacle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def test_saw_osb(Thymio, node, obs_threshold): `  | The class *Thymio* from the file *classes.py* to be able to modify the booleen variable **obs_avoided**, the threashold value to which the sensors detect the obstacle is close enough | returns *True* or *False* depending on if there's an obstacle or not, set the booleen variable **obs_avoided** to *False* if an obstacle is detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **test_saw_obs()** is actually practically the same as *test_saw_wall* from the exercise but without the use of the *verbose* booleen and instead, when an obstacle is detected, the function sets a global booleen variable called **obs_avoided** to *false*. This variable is used in the Local obstacle avoidance function so that it stays in the loop until the obstacle is fully contoured.\n",
    "\n",
    "This function take in input the obstacle threashold (*obs_threshold*) that is set in the 3rd function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_saw_osb(Thymio, node, obs_threshold) :\n",
    "\n",
    "    '''This function verrifies if one of the proximity horiontal sensors\n",
    "        sees and obstacle within the giving threashold.'''\n",
    "    \n",
    "    if any([x>obs_threshold for x in node['prox.horizontal'][:-2]]):\n",
    "\n",
    "        Thymio.obs_avoided = False  # Booleen to state when the obstacle has been avoided or not\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Testing in what direction to contourn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Function | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def clockwise(node):`  |  | returns *True* or *False* depending on if the obstacle is most on the right or the left of the robot and let the next function know weather to contourn clockwise or counterclockwise\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our obstacles are cylinders that the Thymio contourns. Therefore, we want to know whether to contourn it clockwise or counterclockwise. This depends on if the obstacle is more on the right or the left of the robot when it get's in its path. The function **clockwise()** tests which sensor between the sensor <span style='color: red;'>[1]</span> (left) and the sensor <span style='color: red;'>[3]</span> (right) is currently detecting the obstacle. If in fact the sensor on the left ([1]) has a higher value than the other one, the functionn returns *true*, otherwise it returns *false*. This function is only called in the Local obstacle avoidance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clockwise(node) :\n",
    "\n",
    "    '''This function verrifies if the obstacle to avoid is more on its right or left,\n",
    "        therefore, the Thymio will contourn it accordingly.'''\n",
    "\n",
    "    prox = list(node[\"prox.horizontal\"]) + [0]\n",
    "    if prox[1] > prox[3] :\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Local obstacle avoidance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Function| Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def obstacle_avoidance(Thymio, node, client, motor_speed=100, obs_threshold=500):`  | The class *Thymio* from the file *classes.py* to be able to modify the booleen variable **obs_avoided**, the motor speed for the movement of the wheels, set to 100, the threashold value to which the sensors detect the obstacle set to 500 | no real output, but modification of **obs_avoided** booleen when the obstacle is avoided\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **obstacle_avoidance()** can be devided in two states : the '*turning*' and the '*contourning*'.\n",
    "\n",
    "As soon the function is called, it starts by setting two variables : **clockwise_true** to *false* and **prev_state** to \"*turning*\". Then, since the 1st function (**test_saw_osb()**) has been called, the booleen **obs_avoided** is *false* and so we enter a while loop. \n",
    "\n",
    "This is when we enter the state of '*turning*' where we test, thanks to the 2nd function (**clockwise()**) if the Thymio will have to contourn on the left or on the right of the obstacle. If the robot has to turn clockwise, the booleen **clockwise_true** is set to *true*, otherwise, it stays to *false*. The set '*turning*' makes the Thymio rotate on itself so its not facing the obstacle anymore. Once this is done, the state is set to '*contourning*'.\n",
    "\n",
    "Still in the while loop, we enter the state of '*contourning*' where, depending on the value of te booleen **clockwise_true**, the Thymio will move in an arc circle around the obstacle and then back to the '*turning*' state, it rotates on itself again to be back facing in the right direction to allow going on with the Motion Control.\n",
    "\n",
    "At the end of the '*contourning*' state, the booleen variable **obs_avoided** is set to *true*, meaning that the Thymio has finised avoiding the obstacle and that the Motion Control can pick up again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacle_avoidance(Thymio, node, client, motor_speed=100, obs_threshold=500): #, clockwise = False):\n",
    "    \"\"\"\n",
    "    Wall following behaviour of the FSM\n",
    "    param motor_speed: the Thymio's motor speed\n",
    "    param wall_threshold: threshold starting which it is considered that the sensor saw a wall\n",
    "    param white_threshold: threshold starting which it is considered that the ground sensor saw white\n",
    "    param verbose: whether to print status messages or not\n",
    "    \"\"\"\n",
    "\n",
    "    clockwise_true = False  # Booleen to state if the Thymio has to contourn on the left or right\n",
    " \n",
    "    prev_state = \"turning\" # Stated of movement of the Thymio\n",
    "    \n",
    "    while not Thymio.obs_avoided :     # As long as the obstacle isn't avoided, stay in the while loop \n",
    "    \n",
    "        if test_saw_osb(Thymio, node, obs_threshold) :\n",
    "            \n",
    "            if prev_state == \"turning\": # little rotation on it's own to then do the contourning\n",
    "  \n",
    "                if clockwise(node) :\n",
    "                    \n",
    "                    Thymio.setSpeedLeft(motor_speed, node)\n",
    "                    Thymio.setSpeedRight(-motor_speed, node)\n",
    "\n",
    "                    clockwise_true = True   # Thymio needs to contourn the obstacle clockwise\n",
    "\n",
    "                else :\n",
    "                    # Thymio needs to contourn the obstacle counterclockwise\n",
    "                    \n",
    "                    Thymio.setSpeedLeft(-motor_speed, node)\n",
    "                    Thymio.setSpeedRight(motor_speed, node)\n",
    "                    \n",
    "                prev_state = \"contourning\" # Change the state so the Thymio countourns the obstacle fully\n",
    "        \n",
    "        else:\n",
    "            if prev_state == \"contourning\": \n",
    "\n",
    "                if clockwise_true :\n",
    "\n",
    "                    Thymio.setSpeedLeft(motor_speed-40, node)\n",
    "                    Thymio.setSpeedRight(motor_speed, node)\n",
    "\n",
    "                    prev_state = \"turning\"\n",
    "\n",
    "                    aw(client.sleep(18))\n",
    "\n",
    "                    Thymio.setSpeedLeft(motor_speed, node)\n",
    "                    Thymio.setSpeedRight(-motor_speed, node)\n",
    "\n",
    "                    aw(client.sleep(2))\n",
    "\n",
    "                    Thymio.obs_avoided = True  # obstacle has been avoided, change the state booleen\n",
    "                    \n",
    "                else :\n",
    "\n",
    "                    Thymio.setSpeedLeft(motor_speed,node)\n",
    "                    Thymio.setSpeedRight(motor_speed-40,node)\n",
    "\n",
    "                    prev_state=\"turning\"\n",
    "\n",
    "                    aw(client.sleep(18))\n",
    "\n",
    "                    Thymio.setSpeedLeft(-motor_speed,node)\n",
    "                    Thymio.setSpeedRight(motor_speed,node)\n",
    "\n",
    "                    aw(client.sleep(2))\n",
    "\n",
    "                    Thymio.obs_avoided = True  # obstacle has been avoided, change the state booleen\n",
    "\n",
    "        aw(client.sleep(0.1)) #otherwise, variables would not be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. <a id='toc3_5_'></a>[Filtering](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The necessity of a filter comes from the fact that the Thymio is not a perfect robot. This means that the robot may not always act as expected. Therefore, we implement a filter which uses two sources of informations (the position of the Thymio given by the camera and the odometry based on internal measurements specific to the Thymio). The overall goal of the filter is to take into consideration possible imperfections of the robot and predict its actual real position. The **Filtering** module is inspired by the exercise session *Week 8 - Kalman Filter* paragraph 7.1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kalman Filter and sensors used**\n",
    "\n",
    "<u>Filter</u>\n",
    "\n",
    "The issue relies in the incertainty of the exact position of the robot and therefore the accuracy to perform the intended actions. We depend on two sources of information to obtain the position of the Thymio. Hence, from the many possible filters available, we decided to implement a classic Kalman Filter. \n",
    "\n",
    "This filter acts as an estimator for the position and the speed of the Thymio when provided with several source of information. Since the wheels don't have encoders, we use the measurements of the speed and the position provided by the Computer Vision. In addition, with the present noise being Gaussian noise, Kalman filters are a simpler representation of a distribution with the variables : mean and variance, and can be assumed these filters are Gaussian.\n",
    "\n",
    "We also decided to use a classic Kalman Filter rather than a EKF (Extended Kalman Filter) because, even though our system is non linear, we linearize it before applying the Kalman. Therefore, using a classic Kalman is sufficient.\n",
    "\n",
    "The **Filtering** module is inspired by the exercise session *Week 8 - Kalman Filter* paragraph 7.1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<u>Sensors</u>\n",
    "\n",
    "Like mentioned above, since the Kalman Filter is Gaussian, it can be used to compare different sensor measurements : \n",
    "\n",
    "- Camera : from Computer Vision, the information measured is the Thymio's position - coordinates ($x$, $y$) and angle ($\\theta$)\n",
    "- Odometry : using the robot's speed sensors, the informations measured are the wheels speed ($\\dot{x} = v_{x}$, $\\dot{y} = v_{y}$) and the angular velocity ($\\dot{\\theta}$)\n",
    "\n",
    "By using serveral source of measurements, we can allow the filter to function even if the camera is suddently unavailable and have our Motion Control still operate correctly.\n",
    "\n",
    "We will see later on, but it is trough the **Odometry** that we linearize our system so that we can perform a classic KF rather than an EKF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "<u>States</u>\n",
    "\n",
    "The states we consider are the Thymio's position and its velocities. The best case senario is when we have the Computer Vison and the Odometry functionning. \n",
    "\n",
    "$$\n",
    "States \n",
    "= \n",
    "%\\begin{gather}\n",
    " \\begin{bmatrix}\n",
    "           x_{k} \\\\\n",
    "           \\dot{x_{k}}\\\\\n",
    "           y_{k} \\\\\n",
    "           \\dot{y_{k}}\\\\\n",
    "           \\theta_{k} \\\\\n",
    "           \\dot{\\theta_{k}}\n",
    " \\end{bmatrix}\n",
    "=\n",
    " \\begin{bmatrix}\n",
    "           x_{k} \\\\\n",
    "           v_{x_{k}}\\\\\n",
    "           y_{k} \\\\\n",
    "           v_{y_{k}}\\\\\n",
    "           \\theta_{k} \\\\\n",
    "           v_{\\theta_{k}}\n",
    " \\end{bmatrix}\n",
    "%\\end{gather}\n",
    "$$\n",
    "\n",
    "$$x_{t+1} = Ax_{t} + w_{t}$$\n",
    "$$y_{t} = Hx_{t} + v_{t}$$\n",
    "\n",
    "Where, $x_{k}$ is the state of the system at sample $k$, $\\mathbf{A}$ is the matrix defining how the system evolves, $w_{k}$ is the state stochastic perturbation with covariance matrix $\\mathbf{Q}$, $y_{k}$ is the measurement related to the state by the matrix $\\mathbf{H}$ and $v_{k}$ is the measurement noise with covariance matrix $\\mathbf{R}$.\n",
    "\n",
    "The actual measurements $y_{k}$ depend on if we have Camera Vision as well as Odometry or not. On the other hand, we do not use values for known inputs $u_{k}$ and the impacting matrix $\\mathbf{B}$ because we assume that the Thymio's speed is considered as a state and not an input. In other words, we do a sort of approximation of the real system to simplify the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Matrices</u>\n",
    "\n",
    "\n",
    "- Therefore, we start by computing the sensors covariance matrix $\\mathbf{R}$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{R}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "r_{11} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & r_{22} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & r_{33} & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & r_{44} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & r_{55} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & r_{66} \\\\\n",
    "\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "$\n",
    "r_{11} = q_{cam,x} = 7.73 \\quad\n",
    "r_{22} = q_{\\dot{x}} = 6.48\\quad\n",
    "r_{33} = q_{cam,y} = 7.73\\quad\n",
    "r_{44} = q_{\\dot{y}} = 6.48\\quad\n",
    "r_{55} = q_{cam,\\theta} = 0.0049\\quad\n",
    "r_{66} = q_{\\dot{\\theta}} = 0.615\\quad\n",
    "$\n",
    "\n",
    "\n",
    "When there is no vision, the matrix looks like : \n",
    "\n",
    "$$\n",
    "\\mathbf{R}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "r_{22} & 0 & 0 \\\\\n",
    "0 & r_{44} & 0 \\\\\n",
    "0 & 0 & r_{66} \\\\\n",
    "\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "with the same values for $r_{22}$, $r_{44}$ and $r_{66}$\n",
    "\n",
    "The values for $r_{ii}$ are computed, from samples, as the standard deviation of the respective variables ($x_{k}$, $\\dot{x_{k}}$, $y_{k}$, $\\dot{y_{k}}$, $\\theta_{k}$, $\\dot{\\theta_{k}}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the matrix $\\mathbf{R}$, we compute the model covariance matrix $\\mathbf{Q}$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "q_{11} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & q_{22} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & q_{33} & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & q_{44} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & q_{55} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & q_{66} \\\\\n",
    "\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "$\n",
    "q_{11} = q_{cam,x} = 7.73 \\quad\n",
    "q_{22} = q_{\\dot{x}} = 1000\\quad\n",
    "q_{33} = q_{cam,y} = 7.73\\quad\n",
    "q_{44} = q_{\\dot{y}} = 1000\\quad\n",
    "q_{55} = q_{cam,\\theta} = 0.0049\\quad\n",
    "q_{66} = q_{\\dot{\\theta}} = 1000\\quad\n",
    "$\n",
    "\n",
    "The values for $q_{11}$, $q_{33}$ and $q_{55}$ are the same as $r_{11}$, $r_{33}$ and $r_{44}$ because we suppose that the Camera Vision keeps the same error in the model as for the sensors. It gives us the possibility to figure out how wrong the position can become. On the other hand, the values $q_{22}$, $q_{44}$ and $q_{66}$ are set to \"extremely\" high values. This is because the uncertainty of the velocities are high and implies added noise. So, we want that the only way to update the velocities is from the **Odometry**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We compute the matrix that relate the states to the measurements $\\mathbf{H}$ :\n",
    "\n",
    "$$\n",
    "\n",
    "\\mathbf{H_{w/Vision}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\hspace{20 mm}\n",
    "\n",
    "\\mathbf{H_{w/outVision}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the best case senario would be to have the imputs from the two sources of sensors, the camera and the wheels' speed. This allows to have as much information available. In this case, the Kalman Filter can compare the two positions and have a more precise estimation. \n",
    "\n",
    "We decided to use as the second source the wheels speed instead of the accelerometer as internal measurements because we assumed it wouldn't be very precise values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Kalman Filter Function</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def filter_kalman(self, X_est_pre, P_est_pre, Thymio) :`  | no function input but uses global variables from the file classes.py and the modified values from the functino **odometry_update** | no real output but updates values for the estimations of the states and the uncertainty, hence, the updated values for the Thymio's position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we have two situations to take into account when computing the Kalman Filter equations : when we have simultaneously Computer Vision and Odometry, and when we only have Odometry. \n",
    "\n",
    "In the function **filter_kalman**, an  <span style='color: red;'>if / else</span> condition is implemented to distinguish the 2 cases. When the Computer Vision is functionning, it set a booleen variable **vision** to *true*, so that the code enters in the first case, otherwise, it goes in the second case and uses the matrices set to be used in the situation of pure Odometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_kalman(self, Thymio) :\n",
    "     '''code...'''    \n",
    "     if (Thymio.vision) : # If we have the Computer Vision and Odometry\n",
    "\n",
    "          y = [[Thymio.pos_X],[self.v_X],[Thymio.pos_Y],[self.v_Y],[Thymio.theta],[self.v_Theta]] \n",
    "\n",
    "          H = np.eye(6)\n",
    "\n",
    "          R = [[7.73, 0, 0, 0, 0, 0], \n",
    "               [0, 6.48, 0, 0, 0, 0],\n",
    "               [0, 0, 7.73, 0, 0, 0],\n",
    "               [0, 0, 0, 6.48, 0, 0],\n",
    "               [0, 0, 0, 0, 0.0049, 0],\n",
    "               [0, 0, 0, 0, 0, 0.615]]\n",
    "\n",
    "          Thymio.vision = 0\n",
    "     else : # if we only have the Odometry\n",
    "\n",
    "          y = [[self.v_X],[self.v_Y],[self.v_Theta]]\n",
    "\n",
    "          H = [[0, 1, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 1, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 1]]\n",
    "          \n",
    "          R = [[6.48, 0, 0],\n",
    "               [0, 6.48, 0],\n",
    "               [0, 0, 0.615]]\n",
    "     '''...code'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<u>Odometry</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Odometry allows us to linearize the system so we're able to use the classic Kalman Filter rather than an EKF. Through the equations specified below, we go from a non-linear system with the Thymio wheeel speeds $v_{R}$ and $v_{L}$ to a linear system with cartesian speeds $v_{x}$ and $v_{y}$. The latter speeds are the ones used in the **filter_kalman** function to perform the estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Odometry equations\n",
    "\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        v_{x} =\\dot{x} = \\frac{v_{R} + v_{L}}{2} * \\cos\\left (\\theta + \\left (\\frac{\\theta}{2}\\right)\\right) \\\\\n",
    "        \\\\\n",
    "        v_{y} = \\dot{y} =\\frac{v_{R} + v_{L}}{2} * \\sin\\left (\\theta + \\left (\\frac{\\theta}{2}\\right)\\right)\\\\ \n",
    "        \\\\\n",
    "        v_{\\theta} = \\dot{\\theta} = \\frac{v_{R} - v_{L}}{d_{wheels}}\n",
    "    \\end{cases} \n",
    "\n",
    "\n",
    "\\hspace{20 mm}\n",
    "\n",
    "\n",
    "    \\begin{cases}\n",
    "        x_{update} = v_{x} * \\Delta{t}\\\\\n",
    "        \\\\\n",
    "        y_{update} = v_{y} * \\Delta{t}\\\\ \n",
    "        \\\\\n",
    "        \\theta_{update} = v_{\\theta} * \\Delta{t}\n",
    "    \\end{cases} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def odometry_update(self, Thymio) :`  | no function input but uses global variables from the file **classes.py** | no real output but modifies values in the class of **KalmanFilter** for the **filter_kalman** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odometry_update(self, Thymio) :\n",
    "        '''Odometry calculation'''\n",
    "\n",
    "        self.speed_L = Thymio.motor_speed_left\n",
    "        self.speed_R = Thymio.motor_speed_right\n",
    "        \n",
    "        self.wheel_dist = 9.5 # Distance between the wheel (where they touch the ground)\n",
    "\n",
    "        delta_t = time.time() - pre_time            # time.time() to get the value of the time\n",
    "        delta_S = (self.speed_R + self.speed_L) / 2                         # Forward speed\n",
    "        self.v_Theta =  (self.speed_R - self.speed_L) / self.wheel_dist     # Angular velocity \n",
    "\n",
    "        # calculations of the variations of the speed of the Thymio\n",
    "        self.v_X = delta_S * np.cos(Thymio.theta + self.v_Theta/2)          # SPEED in X\n",
    "        self.v_Y = delta_S * np.sin(Thymio.theta + self.v_Theta/2)          # SPEED in y\n",
    "\n",
    "        # update of the position (coordinates and angle/orientation) of the Thymio after time delta_t\n",
    "        self.pos_X_odo = self.v_X * delta_t\n",
    "        self.pos_Y_odo = self.v_Y * delta_t\n",
    "        self.pos_theta_odo = self.v_Theta * delta_t\n",
    "\n",
    "        # Update for the previous time with the current time for the next iteration\n",
    "        pre_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the Kalman Filter seemed to be the best option and the fact that we linearize the system before, using the odometry, we do not need to use the Exteded version even with a non-linear system. \n",
    "\n",
    "We calculated the different parameters for our covariance matrices $\\mathbf{R}$ and $\\mathbf{Q}$ based on measured samples directly taken with the camera and the Thymio's movement.\n",
    "\n",
    "**++impact of the filter on the program, precision etc (illustration avec et sans Kalman et quand camera est cachée)**\n",
    "\n",
    "*The filter has a good impact on the system's precision and we can see that from the illustrations on the camera window. These illustrations come from a function that we deemed interesting to add. It draws ovals with diameters based on the covariance matrice's values in x and y.* \n",
    "\n",
    "*The filter is able to keep an estimation of the position of the robot and correct it, whether the camera is available or not. The prediction is of course more reliable when the camera sends the position of the robot to compute the prediction. We can see that from the oval growing as time passes by when the vision is obstructed.*\n",
    "\n",
    "*Finally, we would say that applying a filter to a small and not so complex system such as the Thymio robot, in an application lasting a few seconds does not have such a impactful influence. However we are now able to understand how important and meaningful to have one on an autonomous system such as a car, a satellite, etc. (basically all linears system with trajectory estimations). During a long-term mission, we cannot expect to cancel the error accumulation, but we can lessen it, hence the usefulness of Bayes filter.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[VIDEOS ? IMAGES ? LITTLE EXTRAS ?](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[Overall Project](#toc0_)\n",
    "\n",
    "**do we have to put the main here ? or a code that called our main.py ? to be asked on tuesday maybe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[Conclusion](#toc0_)\n",
    "\n",
    "**we need to write a conclusion**\n",
    "\n",
    "some difficulties we have faced are entre autre ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
