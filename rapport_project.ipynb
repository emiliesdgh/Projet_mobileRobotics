{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basics of Mobile Robotics project<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "**07.12.2023**\n",
    "\n",
    "**Group 23 : Diana Bejan** (325029)**, Emilie Grandjean** (286734)**, Garance Boesinger** (310447)**, Juan Martín** (376659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Introduction](#toc1_)   \n",
    "\n",
    "2. [General Setup](#toc2_)    \n",
    "2.1. [Project description](#toc2_1_)    \n",
    "2.2. [Environment Setup](#toc2_2_)    \n",
    "2.3. [Best Path Calculations](#toc2_3_)    \n",
    "2.4. [Motion Control](#toc2_4_)    \n",
    "2.5. [Obstacle Avoidance](#toc2_5_)  \n",
    "\n",
    "3. [Required Components](#toc3_)    \n",
    "3.1. [Computer Vision](#toc3_1_)    \n",
    "3.2. [Global Navigation](#toc3_2_)    \n",
    "3.3. [Motion Control](#toc3_3_)    \n",
    "3.4. [Local Navigation](#toc3_4_)    \n",
    "3.5. [Filtering](#toc3_5_)    \n",
    "\n",
    "4. [VIDEOS ? IMAGES ? LITTLE EXTRAS ?](#toc4_)    \n",
    "5. [Overall Project](#toc5_)    \n",
    "6. [Conclusion](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Introduction](#toc0_)\n",
    "\n",
    "As part of the Basics of Mobile Robotic class given by Prof. Mondada, we are asked to implement a project using the Thymio robot. This project involves the robot reaching a goal while moving in an environment filled with permanent obstacles. During it's path, some extra physical obstacles can be placed in its way. In such moments, the Thymio has to be able to avoid them and continue its route to the end point.\n",
    "\n",
    "This is then done using several concepts seen in class such as image processing and pattern detection with Computer Vision, the Motion Control using a controller, Global and Local Navigation of the Thymio and Filtering.\n",
    "\n",
    "In this report, we will talk about the different modules implemented and detail how they work to explain our overall functionnality of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[General Setup](#toc0_)\n",
    "\n",
    "## 2.1. <a id='toc2_1_'></a>[Project description](#toc0_)\n",
    "\n",
    "1. **Create an environnment :** Our environment has to contain a set of obstacles that the Thymio avoids through *global navigation*. That is to say, the Thymio should avoid the obstacles without using the sensors to detect them.\n",
    "\n",
    "2. **Find the best path :** The obective is that the Thymio goes from an *arbitrary point* in the map to a *target* that can be placed <u>anywhere</u> in the environment.  These will be changed during the demo to see how the system performs.\n",
    "\n",
    "3. **Motion Control & Position estimation:** We will *control* the robot to help it move along the path. This requires an accurate estimate of the position of the robot which we will have to obtain through *bayesian filtering*.\n",
    "   \n",
    "4. **Avoid Obstacles :** While navigating, the Thymio will have to use *local navigation* to avoid *physical obstacles* that can be put in its path <u>at any point in time</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[Environment Setup](#toc0_)\n",
    "\n",
    "Our environment setup has 4 ***yellow*** squares as **boundary points** and includes the **goal** (red square), **static obstacles** (black, shapes with sharp lines and edges, they cannot be curvilinear), a **background** (ideally white but it would work with any light color) and the **Thymio's position** (obtained with the colored circles, blue in the front and green in the middle, placed on top of it). The choice of these features is highly related to the vision part as the image captured needs to be processed in the Computer Vision part of our project.\n",
    "\n",
    "As for the static obstacles, we decided to have them in 2D to avoid any shadows that could compromise the detection by the camera. Further explanations on how these obstacles and features are detected are given in the Computer Vision part. \n",
    "\n",
    "An example of a frame captured by the camera with the different features mentioned is illustrated here below : \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. <a id='toc2_3_'></a>[Best Path Calculations](#toc0_)\n",
    "\n",
    "To calculate the best path we chose a visibility graph together with Dijkstra’s algorithm. This is a non-gird-based approach that does not compromise optimality. To compute the algorithm we make use of *Computer Vision* to get the position of the corners of the enlarged obstacles and the start position of the robot so we can later calculate distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. <a id='toc2_4_'></a>[Motion Control](#toc0_)\n",
    "\n",
    "For **Motion Control**, we decided to only set speeds. They are chosen with respect to the distance between the current position to the next point on the path as well as the difference between the current angle and the desired angle. These values are obtained through the Kálmán Filter and the Global Navigation. Essentially, the motion is a continuous exchange between the turning and advancing phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. <a id='toc2_5_'></a>[Obstacle Avoidance](#toc0_)\n",
    "\n",
    "For the Obstacle Avoidance section, we decided to use 3D physical obstacles that cannot be detected by the camera. This will push the use of the Thymio's sensors to detect these obstacles. Since they can be put at any time anywhere in the robot's path, using **Local Navigation** will allow the avoidance. Once the obstacle avoided, the robot needs to go back to following its given path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Required Components](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def fonction()`  | input input input | output output output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Computer Vision](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The **Computer Vision** module is mostly based on the OpenCV documentation. Other sources used are mentioned throughout the explanation. \n",
    "\n",
    "The purpose of the computer vision part, which is autonomous from the other parts but essential to compute the reference points of the setup, is to capture and use the picture from the camera to extract the main features of the environment, analyze them and give the required information to **Global Navigation** and **Odometry** modules (i.e. actual robot position coordinates and angle, the goal and obstacles positions). The particularity of **Computer Vision** module is that it does not communicate with the robot. \n",
    "\n",
    "The selected approach is based on color, shape and corner recognition. The first approach was with \"perfect\" images of a setup created virtually. Once switched to the \"noisy\" pictures of the camera, we had to rescale quite a bit of the values.\n",
    "\n",
    "As for the code implemented, we created a vision class containing the basic arguments computed by **Vision** and the functions that will be called in the main when needed. Some information from **Vision** has to be recomputed at each step, such as the robot position (coordinates and angle), and some of them only need to be computed once in the first step, such as the obstacles' positions and their corners that do not move during execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Image capturing and image filtering**\n",
    "\n",
    "At first, a frame is extracted from the camera. The camera has already been activated in the main (video registered in the variable **cap**). Note that we capture two frames because we noticed that sometimes, during the initial connection of the camera to the computer, the first frame captured is \"yellowish\" and too bright to be used. Then, we filter the image several times to avoid noise and have a \"smoother\" picture to perform the next steps.\n",
    "\n",
    "For the image filtering, we tested several combinations and the one in the function **capture_image()** seems to work best. Therefore, we perform 2D linear filtering, then blur the frame and then apply an additional median blur. Our filtered frame is stored in the vision Class so it can used by all the other vision functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_image(self,cap): \n",
    "    #capture a frame out of the video that will be used through CV part \n",
    "    ret, self.frame = cap.read()\n",
    "    ret, self.frame = cap.read()\n",
    "    kernel = np.ones((5,5),np.float32)/25\n",
    "    img = cv2.filter2D(self.frame,-1,kernel)\n",
    "    img = cv2.blur(img,(5,5))\n",
    "    self.img = cv2.medianBlur(img,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Goal position, thymio position and angle tracking**\n",
    "\n",
    "To find the coordinates of the goal and start position, we use color filtering. We apply a color mask of the target color (red for the goal, blue and green for the robot) on the picture and then use **maxLoc()** function or contour finding to extract the coordinates. The values of the color mask have been found using hsv color space graph and forums, they are empiric values adapted to our setup, and to the colors of the papers we are using. Indeed, one of the main flaws of the vision part, which will be discussed more in detail in the weakness part, is that it is quite sensitive to light and color changes and uniformity.\n",
    "\n",
    "The function **find_goal_pos()** applies a red mask to the filtered frame, isolating the goal shape. Then we apply a contour detection on the thresholded image and find the moment, i.e. center of the shape. This value corresponds to the goal position (center of the red square). \n",
    "\n",
    "The functions **find_start_pos()** and **find_angle()** provide the coordinates of the two points on top of the Thymio. As the circle is of a small radius, we use **maxLoc()** function to compute the coordinates. It is not the exact center of the point but it is precise enough (more or less 10 points on a grid of 480x640). The green point is used as starting point, i.e. as the position of the Thymio. The blue one is used to compute the angle of the Thymio. We obtain a positive angle going from zero to two pi starting from the positive x-axis and turning counterclockwise. We made several measures for the odometry part comparing measured values and real values, and the precision of the angle is +-0.01rad. \n",
    "\n",
    "At the end of the functions computing the positions and angle, we store those values in the shared Thymio Class to be used by the other functions of the code.\n",
    "\n",
    "The **find_angle()** function pasted below computes the coordinates of the blue point on the Thymio first. The coordinates of the green (=start point) are computed in the same way. Then, using both green (back) and blue (front) coordinates, we compute the angle. \n",
    "\n",
    "Below is the thresholded image of the frame given as an example with red mask applied, and of the green point on the robot, along with the coordinates computed by the functions. \n",
    "\n",
    "<center><img src = \"./pictures_report/2.png\" title=“Figure” style=\"height: 250px;\"/>\n",
    "<img src = \"./pictures_report/3.png\" title=“Figure” style=\"height: 250px;\"/></center>\n",
    "\n",
    "<center><i> Figure 2: On the left we see the goal position, on the right we see the start position <i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_angle(self,robot):\n",
    "    # COORDINATES OF THE BLUE POINT \n",
    "    hsv = cv2.cvtColor(self.img, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv, self.LOW_BLUE, self.HIGH_BLUE)\n",
    "    blue = cv2.bitwise_and(self.frame,self.frame, mask = mask)\n",
    "    gray = cv2.cvtColor(blue, cv2.COLOR_RGB2GRAY) \n",
    "    _, self.thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY) \n",
    "    (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(self.thresh1)\n",
    "    [x_front,y_front] = maxLoc\n",
    "    # WE VERIFY ONE BLUE POINT IS ACTUALLY DETECTED TO AVOIR HIDING SITUATIONS (if the robot is hidden, we do not\n",
    "    # recompute the coordinates)\n",
    "    if (x_front != 0) or (y_front != 0):\n",
    "        [self.x_front,self.y_front] = [x_front,y_front]\n",
    "    # ANGLE COMPUTATION\n",
    "        if self.y_front < self.y_back : \n",
    "            if self.x_front > self.x_back : \n",
    "                self.teta = np.arccos((self.x_front-self.x_back)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "            if self.x_front <= self.x_back : \n",
    "                self.teta = np.pi - np.arccos((self.x_back-self.x_front)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "        if self.y_front >= self.y_back : \n",
    "            if self.x_front > self.x_back : \n",
    "                self.teta = 2*np.pi - np.arccos((self.x_front-self.x_back)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "            if self.x_front <= self.x_back : \n",
    "                self.teta = np.pi + np.arccos((self.x_back-self.x_front)/(np.sqrt(np.power((self.x_front-self.x_back),2)+np.power((self.y_front-self.y_back),2))))\n",
    "    # PASS THE VARIABLES TO THE SHARED THYMIO CLASS AND INDICATE THAT VISION IS DONE\n",
    "        robot.setPositions(self.x_back,self.y_back,self.x_goal,self.y_goal,self.teta)\n",
    "        robot.setVisionDone(True)\n",
    "    # If hidden thymio, do not update the vision variables \n",
    "    else:\n",
    "        robot.setVisionDone(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Obstacles**\n",
    "\n",
    "For the obstacles identification part, it depends on the type of global navigation used. As discussed in the next *Gloabl Navigation* section, we implemented two methods for global part, Astar and Visibility.  \n",
    "\n",
    "In the case of visibility graph, the process used is the following:  \n",
    "At first, we apply a big thresholding on the gray image to obtain a frame containing only the obstacles (for a noisy image, we set an empirical value of 40, which keeps only the black obstacles). The corners and moments of the obstacles are obtained using openCV corners and moments finding functions. We then implemented a small part of code placing the corners further away from the shapes to avoid the thymio running into the obstacles when following the side of a shape. The value of the distance of the corners to their shape is a variable that can be changed in the code, and that should typically be half the size of the thymio. We also took out the corners too close to the borders as we do not want the thymio to go outside the setup boundaries. Those methods are implememted in the function *find_corners*.  \n",
    "\n",
    "Then we have to provide the *global navigation* part a \"visibility graph\" with the nodes and weights of each node to all others. Obviously the nodes considered are the corners plus start and goal position. We therefore compute a matrix containing the distance of each point to all others, implemented in the *compute_dist_mx* function. But we cannot just calculate the distance between points because if there is an obstacle in the way between two points, it must not be taken into account. We therefore decided to use threshold on the line to avoid this. Here is a quick explanation of the approach: on the thresholded image of the obstacles, if we extract the pixel values of the line between two points, if it contains black, there is an obstacle in the way and we do not compute the distance. \n",
    "\n",
    "To optimally use this 'line threshold' technique, and before calling compute_dist_mx function, we call the function *trace_contours* that traces the lines beteween the corners to avoid computing path in the \"forbidden area\" between obstacles and corners. \n",
    "\n",
    "Below is a picture of the frame used for this setup to compute the distance matrix, and an example of a distance matrix obtained for the setup provided as illustration. Distance_matrix should be read as follow: indices along vertical and horizontal dimensions are [start_point, corner1, corner2, ..., cornerN, goal_point], matrix_element[i,j] = distance between corner[i] and corner[j], and if there is an obstacle in the path, distance = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corners(self):\n",
    "    # Put the filtered image in grayscale\n",
    "    gray = cv2.cvtColor(self.img, cv2.COLOR_BGR2GRAY) \n",
    "    # Threshold the image to keep only black elements\n",
    "    ret, thresh2 = cv2.threshold(gray, self.BLACK_THRESHOLD, 255, cv2.THRESH_BINARY) \n",
    "    # Find contours of obstacles \n",
    "    contours, _ = cv2.findContours( \n",
    "        thresh2, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    self.thresh2 = thresh2\n",
    "    i = 0\n",
    "    m = []\n",
    "    # Find moments, i.e. centers of obstacles \n",
    "    for contour in contours:  \n",
    "        if i == 0: \n",
    "            i = 1\n",
    "            continue\n",
    "        M = cv2.moments(contour) \n",
    "        if M['m00'] != 0.0: \n",
    "            x = int(M['m10']/M['m00']) \n",
    "            y = int(M['m01']/M['m00']) \n",
    "            if len(m)<self.NB_SHAPES:\n",
    "                m.append([x,y])\n",
    "    th3 = cv2.adaptiveThreshold(self.thresh2,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "                                cv2.THRESH_BINARY,11,2)\n",
    "    # Find corners of obstacles \n",
    "    corners = cv2.goodFeaturesToTrack(th3, self.NB_CORNERS, 0.01, 45) \n",
    "    corners = np.int0(corners) \n",
    "    # Part of code that changes the coordinates of the corners to put them away from the obstacles \n",
    "    \n",
    "    ####*** CODE TO PUT THE CORNERS AWAY FROM THE OBSTACLES ***####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5.png](attachment:5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_contours(self):\n",
    "    a = int(np.size(self.cornerss)/2)\n",
    "    b = int(np.size(self.m_cor)/2)\n",
    "    for i in range (0,a):\n",
    "        for j in range (0,b):\n",
    "            if i != j:\n",
    "                if self.m_cor[i] == self.m_cor[j]:\n",
    "                    line = np.transpose(np.array(draw.line(self.cornerss[i][0], self.cornerss[i][1], self.cornerss[j][0], self.cornerss[j][1])))\n",
    "                    data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "                    if np.size(np.where(abs(np.diff(data))>0)[0]) <= 2:\n",
    "                        if np.mean(data) > self.mean_value_along_line:\n",
    "                            cv2.line(self.thresh2, self.cornerss[i], self.cornerss[j], self.bluepx, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6.png](attachment:6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_mx(self,robot):\n",
    "    s = int(((np.size(self.cor))/2)+2) \n",
    "    dist_mx = np.zeros((s,s))\n",
    "    for i in range(1,s):\n",
    "        if i == (s-1):\n",
    "            line = np.transpose(np.array(draw.line(self.x_back,self.y_back, self.x_goal,self.y_goal)))\n",
    "            data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "            if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                dist_mx[i][0] = np.sqrt(np.power((self.x_goal-self.x_back),2)+np.power((self.y_goal-self.y_back),2))\n",
    "            continue \n",
    "        line = np.transpose(np.array(draw.line(self.x_back,self.y_back,self.cor[i-1][0],self.cor[i-1][1])))\n",
    "        data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "        if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "            dist_mx[i][0] = np.sqrt(np.power((self.cor[i-1][0]-self.x_back),2)+np.power((self.cor[i-1][1]-self.y_back),2))\n",
    "    for i in range(0,(s-2)):\n",
    "        for j in range(0,s):\n",
    "            if j == (i+1):\n",
    "                continue \n",
    "            if j == 0 : \n",
    "                line = np.transpose(np.array(draw.line(self.x_back,self.y_back, self.cor[i][0],self.cor[i][1])))\n",
    "                data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "                if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                    dist_mx[j][i+1] = np.sqrt(np.power((self.cor[i][0]-self.x_back),2)+np.power((self.cor[i][1]-self.y_back),2))\n",
    "                continue \n",
    "            if j == int(s-1):\n",
    "                line = np.transpose(np.array(draw.line(self.x_goal,self.y_goal, self.cor[i][0],self.cor[i][1])))\n",
    "                data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "                if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                    dist_mx[j][i+1] = np.sqrt(np.power((self.cor[i][0]-self.x_goal),2)+np.power((self.cor[i][1]-self.y_goal),2))\n",
    "                continue\n",
    "            line = np.transpose(np.array(draw.line(self.cor[j-1][0],self.cor[j-1][1],self.cor[i][0],self.cor[i][1])))\n",
    "            data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "            if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                dist_mx[j][i+1] = np.sqrt(np.power((self.cor[i][0]-self.cor[j-1][0]),2)+np.power((self.cor[i][1]-self.cor[j-1][1]),2))\n",
    "    for i in range(0,(s-1)):\n",
    "        if i == 0:\n",
    "            line = np.transpose(np.array(draw.line(self.x_back,self.y_back, self.x_goal,self.y_goal)))\n",
    "            data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "            if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "                dist_mx[0][s-1] = np.sqrt(np.power((self.x_goal-self.x_back),2)+np.power((self.y_goal-self.y_back),2))\n",
    "            continue \n",
    "        line = np.transpose(np.array(draw.line(self.x_goal,self.y_goal,self.cor[i-1][0],self.cor[i-1][1])))\n",
    "        data = self.thresh2[line[:, 1], line[:, 0]]\n",
    "        if np.size(np.where(abs(np.diff(data))>0)[0]) <= 3 : \n",
    "            dist_mx[i][s-1] = np.sqrt(np.power((self.cor[i-1][0]-self.x_goal),2)+np.power((self.cor[i-1][1]-self.y_goal),2))\n",
    "    self.dist_mx = dist_mx\n",
    "    robot.setDistMx(dist_mx)\n",
    "    robot.setCor(self.cor)\n",
    "    robot.setS(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a distance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the vision work for the Astar algorithm, we provide the global navigation part an occupancy matrix, containing 0 where the space is free and 1 where there is an obstacle. We first need to resize the image because Astar needs a smaller grid to operate quickly enough. We can set the resizing parameters in the code.  \n",
    "\n",
    "We use the obstacle image and transform it into a matrix, and then apply a threshold on the values of the matrix to obtain a binary 0/1 matrix, and store the occupancy matrix in the shared robot Class instance. This is implemented in the function **return_occupancy_matrix()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_occupancy_matrix(self,robot):\n",
    "    dim = (self.width_resized, self.height_resized)\n",
    "    img = cv2.resize(self.img, dim, interpolation = cv2.INTER_AREA)\n",
    "    rszd = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mtx = np.array(rszd)\n",
    "    mx = (mtx < 20).astype(int)\n",
    "    robot.occupancy_matrix = mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of the flaws of the computer vision part**\n",
    "\n",
    "- The color filtering is fun and useful but with hsv space and masking, we explore a \"range\" of values (for example \"reddish\" values). In noisy pictures, if the range is too large, we will detect a little bit of this color somewhere it should not be, and on the contrary if the range is too small we will not detect the target, or we will detect it in one lightning condition and not the other. To have an optimal color detection, we should have \"perfect\" pictures which, in reality, do not exist. It is therefore an issue to be dealt with. In our case, once the picture was well filtered and the mask were adapted to the colors we used, it worked well in most situations. \n",
    "\n",
    "- The functions implementing the distance matrix, putting away corners and thresholding along lines have been computed fully \"by hand\" and are surely not optimized. Using existing libraries and tools, we could optimize those functions and clean them out.\n",
    "\n",
    "- The visibility part is very sensitive to lighting conditions.\n",
    "\n",
    "- Our setup and code would not work with faded colors, a bright light spot somewhere on the frame, an artificial colored light or unidentified obstacles.\n",
    "\n",
    "- Our approach does not support round shaped obstacles.\n",
    "\n",
    "- The algorithm needs to be provided the exact number of shapes and corners before the execution. Those variables can nonetheless be easily modified in the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of Vision in the main**\n",
    "\n",
    "At the beginning of the execution, a new object of *vision Class* is created, along with a capture of the video taken by the camera. In the infinity loop, the vision functions are always called first as those values are needed for the rest of the program. It is implememted as follows:\n",
    "\n",
    "- vision Class object initialized outside the while loop at the beginning of the execution.\n",
    "\n",
    "- initialization of the video captured from the camera.\n",
    "\n",
    "In the infinite loop:  \n",
    "- Every time, we capture a new frame, find the goal and the Thymio's position and angle. \n",
    "\n",
    "- We only compute the contours and corners of the obstacles once.\n",
    "\n",
    "- The distance matrix for the visibility graph (+ path recomputing discussed in the global navigation part) is computed the first time we execute the loop, plus everytime we are in a kidnapping situation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Classes initialization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Thymio\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiltering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KalmanFilter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vision\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglobal_visibility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Global_Nav\n",
      "File \u001b[0;32m~/Projet_mobileRobotics/Projet_mobileRobotics/filtering.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Thymio\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vision\n\u001b[1;32m      7\u001b[0m Thymio_speedTo_mm_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m      8\u001b[0m speed_ratio \u001b[38;5;241m=\u001b[39m Thymio_speedTo_mm_ratio \u001b[38;5;241m*\u001b[39m Vision()\u001b[38;5;241m.\u001b[39mORIGINAL_DIM[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m952\u001b[39m\n",
      "File \u001b[0;32m~/Projet_mobileRobotics/Projet_mobileRobotics/vision.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m colors\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Thymio\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_output\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "#Classes initialization\n",
    "from classes import Thymio\n",
    "from filtering import KalmanFilter\n",
    "from vision import Vision\n",
    "from global_visibility import Global_Nav\n",
    "\n",
    "robot = Thymio() # Set Thym as class Thymio as initialization before the while\n",
    "KF = KalmanFilter()\n",
    "vision = Vision()\n",
    "global_nav = Global_Nav()\n",
    "\n",
    "#Video capturing \n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "a = 0\n",
    "while(1) :\n",
    "    a = a + 1\n",
    "    #VISION\n",
    "    vision.capture_image(cap)\n",
    "    vision.find_goal_pos()\n",
    "    vision.find_start_pos(robot,a)\n",
    "    vision.find_angle(robot)\n",
    "    if (a == 1) : \n",
    "        vision.find_corners()\n",
    "        vision.trace_contours()\n",
    "    if ((robot.vision == 1) & (a == 1))or((a != 1) & (robot.kidnap == True) & (robot.vision == 1)):\n",
    "        vision.compute_dist_mx(robot)\n",
    "        global_nav.dijkstra(robot)\n",
    "        global_nav.extract_path(robot)\n",
    "        if robot.kidnap == True:\n",
    "            robot.kidnap = False\n",
    "    robot.vision = True\n",
    "    #END VISION \n",
    "    \n",
    "    ###***REST OF THE CODE***###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources :**\n",
    "\n",
    "TP2 and 'Project Computer Vision tutorial' of the *Basics of mobile robotic* course  \n",
    "OpenCV documentation: https://docs.opencv.org/4.x/  \n",
    "https://www.geeksforgeeks.org/how-to-detect-shapes-in-images-in-python-using-opencv/  \n",
    "https://www.geeksforgeeks.org/python-detect-corner-of-an-image-using-opencv/  \n",
    "https://stackoverflow.com/questions/30331944/finding-red-color-in-image-using-python-opencv  \n",
    "https://www.delftstack.com/fr/howto/python/opencv-inrange/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Global Navigation](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The **Global Navigation** module goal is to provide a path for the **Motion Control** module, based on the informations provided by the **Vision** module. We implemented the visibility graph, which will be explained hereinafter. We use the distance matrix between the points provided in the robot class instance and compute the best path using Dijkstra's algorithm in the function **dijkstra()**, based on the pseudo-code we found here: https://www.baeldung.com/cs/dijkstra#:~:text=Dijkstra's%20Algorithm%20is%20a%20pathfinding,we%20reach%20the%20end%20node.  \n",
    "\n",
    "The output of this function is a *graph* matrix containing, for each node, its state ('visited' or not), its score to the start point and, most importantly, its nearest node (to the start).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra(self,robot):\n",
    "\n",
    "    self.dist_mx = robot.getDistMx()\n",
    "    self.s = robot.getS()\n",
    "    graph = np.zeros((3,self.s))\n",
    "    graph[0] = 3000 \n",
    "    graph[0][0] = 0\n",
    "    graph[2] = 42\n",
    "    result = 0\n",
    "    current_node = 0\n",
    "    a = 0\n",
    "    while 1:\n",
    "        b = 0\n",
    "        for i in range (0,self.s):\n",
    "            if graph[1][i] == 0: \n",
    "                b = b + 1\n",
    "                if graph[0][i] < graph[0][result]:\n",
    "                    result = i\n",
    "                if b == 1:\n",
    "                    result = i\n",
    "        current_node = result \n",
    "        graph[1][current_node] = 1\n",
    "        for i in range (0,self.s):\n",
    "            if self.dist_mx[i][current_node] != 0:\n",
    "                if graph[1][i] == 0: \n",
    "                    new_score = graph[0][current_node] + self.dist_mx[i][current_node]\n",
    "                    if new_score < graph[0][i]:\n",
    "                        graph[0][i] = new_score \n",
    "                        graph[2][i] = current_node \n",
    "        if current_node == int(self.s-1):\n",
    "            break\n",
    "        for i in range (0,self.s):\n",
    "            if graph[1][i] == 0: \n",
    "                if graph[0][i] < graph[0][result]:\n",
    "                    result = i\n",
    "                    if graph[0][result] == 3000:\n",
    "                        break\n",
    "\n",
    "    self.graph = graph \n",
    "    self.current_node = current_node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the function **extract_path()** to compute in a single array the coordinates of the path points, from start point (Thymio) to goal. This is done by extracting the coordinates using *graph* matrix, going from goal through each 'closest node' until start position. We then put this path in the shared robot class to be used by the **Motion Control** module.  \n",
    "\n",
    "Below is an example of a path computed by the Visibility Graph algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(self,robot):\n",
    "    cor = robot.getCor()\n",
    "    path = [self.current_node]\n",
    "    node = self.current_node\n",
    "    while node != 0:\n",
    "        path.append(int(self.graph[2][node]))\n",
    "        node = path[-1]\n",
    "    path = np.flip(path)\n",
    "    path_coord = [[robot.pos_X,robot.pos_Y]]\n",
    "    a = 0\n",
    "    for p in path : \n",
    "        a = a + 1 \n",
    "        if a != 1:\n",
    "            if a != np.size(path):\n",
    "                i = int(p-1)\n",
    "                path_coord.append(cor[i])\n",
    "    path_coord.append([robot.goal_X,robot.goal_Y])\n",
    "    robot.path = path_coord\n",
    "    if len(path_coord)>1:\n",
    "        robot.setAngle()\n",
    "    self.path = path \n",
    "    self.path_coord = path_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![7.png](attachment:7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[Motion Control](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "This module of our project is used to give to the wheels the necessary speeds to make the right movements. Three functions are present : the P controller **PControl()**, the turning function **turn()**, and the forward motion function **go_to_next_point()**. We switch between two phases while going from one point to the next. First, the turning phase where we use **turn()** to get in the angle that allows the robot to only move forward to reach the next point. Second, the advancing phase, which advences to the next point. \n",
    "\n",
    "The module is only called when there is no obstacle detected by the robot's proximity sensors. The functions work in a point-to-point and moment-to-moment fashion. This means that the angles taken into account are the current angle given by the Kálmán filter as well as the goal angle calculated between the first and second points of the path. The speeds are calculated for the distance between the two aforementioned points rather than from the current position to the final goal position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. P Controller**\n",
    "\n",
    "The P controller is used for angle correction and is an integral part of both the turning and the forward motion phases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def PController()`  | the error between the current angle and the desired one <br> the object of class Thymio that the changes apply to| the rotational speed to give to the wheels |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, we look at the actual angle, *current_angle*, given by the filter,  and the goal angle of the robot, *robot.goal_angle* and using the difference between the two, *error*, to calculate an optimal speed for attaining the desired angle. \n",
    "\n",
    "While in **turn()** it is the only calculation of speed, in **go_to_next_point()** it adds a rotational speed to a forward speed already calculated. The coefficient KP is set by doing experiments during some testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Turning function**\n",
    "\n",
    "To go from a starting point to the next one, before even moving forwards, the first step is to orient the robot in the right direction. This is done with the function **turn()**.It takes as numerical inputs the current angle, given by the filter, and the goal angle of the robot, calculated from the path in the Thymio class. \n",
    "\n",
    "The P controller is then called for the calculation of the speeds that are given to the robot by the functions **setSpeedLeft()/setSpeedRight()** found in the class Thymio. A helper variable, *goal_reached_t* is used to signal when the turning phase has ended and the forward motion can begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def turn()`  | the current angle, the object of class Thymio that the changes apply to, the node which the information is sent to | none, the setting of the speeds is done internally |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Forward motion function**\n",
    "\n",
    "Once the turning phase is done, signalled by *goal_reached_t*, the advancing phase can start. This function calculates and sends speeds similar to the turning phase. The speed is proportionally controlled to slow down and smoothly reach the next point. The coefficient *K* has also been experimentally chosen during testing. We use the P controller to add a rotational speed in order to correct the angle and so, make sure that the Thymio stays on the right track.\n",
    "\n",
    "Once the next point is reached, it is discarded, hence decreasing the length of the path, and a new goal angle is calculated using the **setAngle()** function found in the file *classes.py*. A helper variable *goal_reached_f* is used to signal that the motion phase has ended and a new turning motion can begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def go_to_next_point()`  | the current angle, the current_position, if an obstacle is detected, the node which the information is sent to | none, only internal changes and speed setting|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Management of the reaching of the final goal position**\n",
    "\n",
    "When the length of the path is down to one, this means the goal position has been reached and the work is done. We see this in the code for **go_to_next_point()**, where there is a condition that skips the function if the path length is too short. The speeds are set to 0 as a advancing phase has been completed, and no new angle is calculated nor is the path changed anymore, as only the current position is left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_to_next_point(current_angle, current_position, obstacle, robot, node): \n",
    "    if len(robot.path) > 1:\n",
    "        '''code...'''\n",
    "    else:   # Advancing phase in completed when close enough to the desired point\n",
    "        robot.goal_reached_f = True\n",
    "        robot.goal_reached_t = False\n",
    "        robot.prev_error = 0\n",
    "        robot.int_error = 0\n",
    "        robot.prev_time = 0\n",
    "        robot.setSpeedRight(0,node)\n",
    "        robot.setSpeedLeft(0,node)\n",
    "        robot.path.pop(1)   # Path shortened\n",
    "\n",
    "    if len(robot.path) > 1:\n",
    "        robot.setAngle(current_position[0],current_position[1])    # Angle changed with respect to the new goal\n",
    "                \n",
    "    else:\n",
    "        pass    # Function skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the code above, the condition has to be introduced in the advancing phase completion as the **robot.setAngle()** function takes the shortened path as an input before it is taken into account by the whole function in a future call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[Local Navigation](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The **Local Navigation** module is inspired by the exercise session *Week 3 - Artificial Neural Networks* paragraph 5. The local_navigation.py file contains 3 functions, two verification functions and the actual function that makes the Thymio avoid obstacles which would be placed in its path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Testing the presence of an obstacle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   ` def test_saw_osb(Thymio, node, obs_threshold): `  | The class *Thymio* from the file *classes.py* to be able to modify the booleen variable **obs_avoided**, the threashold value to which the sensors detect the obstacle is close enough | returns *True* or *False* depending on if there's an obstacle or not, set the booleen variable **obs_avoided** to *False* if an obstacle is detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **test_saw_obs()** is actually practically the same as *test_saw_wall* from the exercise but without the use of the *verbose* booleen and instead, when an obstacle is detected, the function sets a global booleen variable called **obs_avoided** to *false*. This variable is used in the Local obstacle avoidance function so that it stays in the loop until the obstacle is fully contoured.\n",
    "\n",
    "This function take in input the obstacle threashold (*obs_threshold*) that is set in the 3rd function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_saw_osb(Thymio, node, obs_threshold) :\n",
    "\n",
    "    '''This function verrifies if one of the proximity horiontal sensors\n",
    "        sees and obstacle within the giving threashold.'''\n",
    "    \n",
    "    if any([x>obs_threshold for x in node['prox.horizontal'][:-2]]):\n",
    "\n",
    "        Thymio.obs_avoided = False  # Booleen to state when the obstacle has been avoided or not\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Testing in what direction to contourn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Function | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def clockwise(node):`  |  | returns *True* or *False* depending on if the obstacle is most on the right or the left of the robot and let the next function know weather to contourn clockwise or counterclockwise\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our obstacles are cylinders that the Thymio contourns. Therefore, we want to know whether to contourn it clockwise or counterclockwise. This depends on if the obstacle is more on the right or the left of the robot when it get's in its path. The function **clockwise()** tests which sensor between the sensor <span style='color: red;'>[1]</span> (left) and the sensor <span style='color: red;'>[3]</span> (right) is currently detecting the obstacle. If in fact the sensor on the left ([1]) has a higher value than the other one, the functionn returns *true*, otherwise it returns *false*. This function is only called in the Local obstacle avoidance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clockwise(node) :\n",
    "\n",
    "    '''This function verrifies if the obstacle to avoid is more on its right or left,\n",
    "        therefore, the Thymio will contourn it accordingly.'''\n",
    "\n",
    "    prox = list(node[\"prox.horizontal\"]) + [0]\n",
    "    if prox[1] > prox[3] :\n",
    "        return True # returns True if the obstacle is closer to the sensor [1] rather than the sensor [3]\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Local obstacle avoidance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Function| Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def obstacle_avoidance(Thymio, node, client, motor_speed=100, obs_threshold=500):`  | The class *Thymio* from the file *classes.py* to be able to modify the booleen variable **obs_avoided**, the motor speed for the movement of the wheels, set to 100, the threashold value to which the sensors detect the obstacle set to 500 | no real output, but modification of **obs_avoided** booleen when the obstacle is avoided\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **obstacle_avoidance()** can be devided in two states : the '*turning*' and the '*contourning*'.\n",
    "\n",
    "As soon the function is called, it starts by setting two variables : **clockwise_true** to *false* and **prev_state** to \"*turning*\". Then, since the 1st function (**test_saw_osb()**) has been called, the booleen **obs_avoided** is *false* and so we enter a while loop. \n",
    "\n",
    "This is when we enter the state of '*turning*' where we test, thanks to the 2nd function (**clockwise()**) if the Thymio will have to contourn on the left or on the right of the obstacle. If the robot has to turn clockwise, the booleen **clockwise_true** is set to *true*, otherwise, it stays to *false*. The set '*turning*' makes the Thymio rotate on itself so its not facing the obstacle anymore. Once this is done, the state is set to '*contourning*'.\n",
    "\n",
    "Still in the while loop, we enter the state of '*contourning*' where, depending on the value of te booleen **clockwise_true**, the Thymio will move in an arc circle around the obstacle and then back to the '*turning*' state, it rotates on itself again to be back facing in the right direction to allow going on with the Motion Control.\n",
    "\n",
    "At the end of the '*contourning*' state, the booleen variable **obs_avoided** is set to *true*, meaning that the Thymio has finised avoiding the obstacle and that the Motion Control can pick up again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. <a id='toc3_5_'></a>[Filtering](#toc0_)\n",
    "\n",
    "<u>**Idea**</u>\n",
    "\n",
    "The necessity of a filter comes from the fact that the Thymio is not a perfect robot. This means that the robot may not always act as expected. Therefore, we implement a filter which uses two sources of informations (the position of the Thymio given by the camera and the odometry based on internal measurements specific to the Thymio). The overall goal of the filter is to take into consideration possible imperfections of the robot and predict its actual real position. The **Filtering** module is inspired by the exercise session *Week 8 - Kalman Filter* paragraph 7.1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kalman Filter and sensors used**\n",
    "\n",
    "<u>Filter</u>\n",
    "\n",
    "The issue relies in the incertainty of the exact position of the robot and therefore the accuracy to perform the intended actions. We depend on two sources of information to obtain the position of the Thymio. Hence, from the many possible filters available, we decided to implement a classic Kalman Filter. \n",
    "\n",
    "This filter acts as an estimator for the position and the speed of the Thymio when provided with several source of information. Since the wheels don't have encoders, we use the measurements on the speed of the wheels and the position provided by the Computer Vision. In addition, with the present noise being Gaussian noise, Kalman filters are a simpler representation of a distribution with the variables : mean and variance, and can be assumed these filters are Gaussian.\n",
    "\n",
    "We also decided to use a classic Kalman Filter rather than a EKF (Extended Kalman Filter) because, even though our system is non linear, we linearize it before applying the Kalman. Therefore, using a classic Kalman is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<u>Sensors</u>\n",
    "\n",
    "Like mentioned above, since the Kalman Filter is Gaussian, it can be used to compare different sensor measurements : \n",
    "\n",
    "- Camera : from Computer Vision, the information measured is the Thymio's position - coordinates ($x$, $y$) and angle ($\\theta$)\n",
    "- Odometry : using the robot's speed sensors, the informations measured are the wheels speed ($\\dot{x} = v_{x}$, $\\dot{y} = v_{y}$) and the angular velocity ($\\dot{\\theta}$)\n",
    "\n",
    "By using serveral source of measurements, we can allow the filter to function even if the camera is suddently unavailable and have our Motion Control still operate correctly.\n",
    "\n",
    "We will see later on, but it is trough the **Odometry** that we linearize our system so that we can perform a classic KF rather than an EKF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "<u>States</u>\n",
    "\n",
    "The states we consider are the Thymio's position and its velocities. The best case senario is when we have the Computer Vison and the Odometry functionning. \n",
    "\n",
    "$$\n",
    "States \n",
    "= \n",
    "%\\begin{gather}\n",
    " \\begin{bmatrix}\n",
    "           x_{k} \\\\\n",
    "           \\dot{x_{k}}\\\\\n",
    "           y_{k} \\\\\n",
    "           \\dot{y_{k}}\\\\\n",
    "           \\theta_{k} \\\\\n",
    "           \\dot{\\theta_{k}}\n",
    " \\end{bmatrix}\n",
    "=\n",
    " \\begin{bmatrix}\n",
    "           x_{k} \\\\\n",
    "           v_{x_{k}}\\\\\n",
    "           y_{k} \\\\\n",
    "           v_{y_{k}}\\\\\n",
    "           \\theta_{k} \\\\\n",
    "           v_{\\theta_{k}}\n",
    " \\end{bmatrix}\n",
    "%\\end{gather}\n",
    "$$\n",
    "\n",
    "$$x_{t+1} = Ax_{t} + w_{t}$$\n",
    "$$y_{t} = Hx_{t} + v_{t}$$\n",
    "\n",
    "Where, $x_{k}$ is the state of the system at sample $k$, $\\mathbf{A}$ is the matrix defining how the system evolves, $w_{k}$ is the state stochastic perturbation with covariance matrix $\\mathbf{Q}$, $y_{k}$ is the measurement related to the state by the matrix $\\mathbf{H}$ and $v_{k}$ is the measurement noise with covariance matrix $\\mathbf{R}$.\n",
    "\n",
    "The actual measurements $y_{k}$ depend on if we have Camera Vision as well as Odometry or not. On the other hand, we do not use values for known inputs $u_{k}$ and the impacting matrix $\\mathbf{B}$ because we assume that the Thymio's speed is considered as a state and not an input. In other words, we do a sort of approximation of the real system to simplify the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Matrices</u>\n",
    "\n",
    "\n",
    "- Therefore, we start by computing the sensors covariance matrix $\\mathbf{R}$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{R}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "r_{11} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & r_{22} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & r_{33} & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & r_{44} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & r_{55} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & r_{66} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "$\n",
    "r_{11} = q_{cam,x} = 0.337 \\quad\n",
    "r_{22} = q_{\\dot{x}} = 6.48\\quad\n",
    "r_{33} = q_{cam,y} = 0.337\\quad\n",
    "r_{44} = q_{\\dot{y}} = 6.48\\quad\n",
    "r_{55} = q_{cam,\\theta} = 0.01\\quad\n",
    "r_{66} = q_{\\dot{\\theta}} = 0.615\\quad\n",
    "$\n",
    "\n",
    "\n",
    "When there is no vision, the matrix looks like : \n",
    "\n",
    "$$\n",
    "\\mathbf{R}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "r_{22} & 0 & 0 \\\\\n",
    "0 & r_{44} & 0 \\\\\n",
    "0 & 0 & r_{66} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "with the same values for $r_{22}$, $r_{44}$ and $r_{66}$\n",
    "\n",
    "The values for $r_{ii}$ are computed, from samples, as the standard deviation of the respective variables ($x_{k}$, $\\dot{x_{k}}$, $y_{k}$, $\\dot{y_{k}}$, $\\theta_{k}$, $\\dot{\\theta_{k}}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the matrix $\\mathbf{R}$, we compute the model covariance matrix $\\mathbf{Q}$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "q_{11} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & q_{22} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & q_{33} & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & q_{44} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & q_{55} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & q_{66} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "$\n",
    "q_{11} = q_{cam,x} = 0.337 \\quad\n",
    "q_{22} = q_{\\dot{x}} = 6.48\\quad\n",
    "q_{33} = q_{cam,y} = 0.337\\quad\n",
    "q_{44} = q_{\\dot{y}} = 6.48\\quad\n",
    "q_{55} = q_{cam,\\theta} = 0.01\\quad\n",
    "q_{66} = q_{\\dot{\\theta}} = 0.615\\quad\n",
    "$\n",
    "\n",
    "The values for $q_{ii}$ and $r_{ii}$ (with i = 1 to 6) are the same, because we suppose that we trust our system as much as our model. The values of the matrix \\mathbf{Q} can be tuned to be a little bit smaller for the velocities if we would rather trust our model more than ou system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We compute the matrix that relate the states to the measurements $\\mathbf{H}$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{H_{w/Vision}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\hspace{20 mm}\n",
    "\\mathbf{H_{w/outVision}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the best case senario would be to have the imputs from the two sources of sensors, the camera and the wheels' speed. This allows to have as much information available. In this case, the Kalman Filter can compare the two positions and have a more precise estimation. \n",
    "\n",
    "We decided to use as the second source the wheels speed instead of the accelerometer as internal measurements because we assumed it wouldn't be very precise values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Kalman Filter Function</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def filter_kalman(self, Thymio) :`  | no function input but uses global variables from the file classes.py and the modified values from the function **odometry_update()** | no real output but updates values for the estimations of the states and the uncertainty, hence, the updated values for the Thymio's position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we have two situations to take into account when computing the Kalman Filter equations : when we have simultaneously Computer Vision and Odometry, and when we only have Odometry. \n",
    "\n",
    "In the function **filter_kalman()**, an  <span style='color: red;'>if / else</span> condition is implemented to distinguish the 2 cases. When the Computer Vision is functionning, it set a booleen variable **vision** to *true*, so that the code enters in the first case, otherwise, it goes in the second case and uses the matrices set to be used in the situation of pure Odometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_kalman(self, Thymio) :\n",
    "     '''code...'''     \n",
    "     if (Thymio.vision) : # If we have the Computer Vision and Odometry\n",
    "\n",
    "          y = np.array([[Thymio.pos_X],\n",
    "                         [self.v_X],\n",
    "                         [Thymio.pos_Y],\n",
    "                         [self.v_Y],\n",
    "                         [Thymio.theta],\n",
    "                         [self.v_Theta]])\n",
    "\n",
    "          H = np.eye(6)\n",
    "\n",
    "          R = np.array([[0.337, 0, 0, 0, 0, 0], \n",
    "                         [0, 6.48,  0, 0, 0, 0],\n",
    "                         [0, 0, 0.337, 0, 0, 0],\n",
    "                         [0, 0, 0, 6.48,  0, 0],\n",
    "                         [0, 0, 0, 0, 0.01,  0],\n",
    "                         [0, 0, 0, 0, 0, 0.615]])\n",
    "\n",
    "     else : # if we only have the Odometry\n",
    "\n",
    "          y = np.array([[self.v_X],\n",
    "                         [self.v_Y],\n",
    "                         [self.v_Theta]])\n",
    "\n",
    "          H = np.array([[0, 1, 0, 0, 0, 0],\n",
    "                         [0, 0, 0, 1, 0, 0],\n",
    "                         [0, 0, 0, 0, 0, 1]])\n",
    "          \n",
    "          R = np.array([[6.48, 0, 0],\n",
    "                         [0, 6.48, 0],\n",
    "                         [0, 0, 0.615]])\n",
    "     '''...code'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Odometry</u>\n",
    "\n",
    "The Odometry allows us to linearize the system so we're able to use the classic Kalman Filter rather than an EKF. Through the equations specified below, we go from a non-linear system with the Thymio wheeel speeds $v_{R}$ and $v_{L}$ to a linear system with cartesian speeds $v_{x}$ and $v_{y}$. The latter speeds are the ones used in the **filter_kalman** function to perform the estimations.\n",
    "\n",
    "- Odometry equations\n",
    "\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        v_{x} =\\dot{x} = \\frac{v_{R} + v_{L}}{2} * \\cos\\left (\\theta + \\left (\\frac{\\theta}{2}\\right)\\right) \\\\\n",
    "        \\\\\n",
    "        v_{y} = \\dot{y} =\\frac{v_{R} + v_{L}}{2} * \\sin\\left (\\theta + \\left (\\frac{\\theta}{2}\\right)\\right)\\\\ \n",
    "        \\\\\n",
    "        v_{\\theta} = \\dot{\\theta} = \\frac{v_{R} - v_{L}}{d_{wheels}}\n",
    "    \\end{cases} \n",
    "\\hspace{20 mm}\n",
    "    \\begin{cases}\n",
    "        x_{update} = v_{x} * \\Delta{t}\\\\\n",
    "        \\\\\n",
    "        y_{update} = v_{y} * \\Delta{t}\\\\ \n",
    "        \\\\\n",
    "        \\theta_{update} = v_{\\theta} * \\Delta{t}\n",
    "    \\end{cases} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Funtion | Input | Output |\n",
    "|:------|:------|:------|\n",
    "|   `def odometry_update(self, Thymio) :`  | no function input but uses global variables from the file **classes.py** | no real output but modifies values in the class of **KalmanFilter** for the **filter_kalman()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odometry_update(self, Thymio, node) :\n",
    "        '''Odometry calculation'''\n",
    "\n",
    "        Thymio.getSpeeds(node)\n",
    "\n",
    "        self.real_speed_L = Thymio.motor_left_speed \n",
    "        self.real_speed_R = Thymio.motor_right_speed \n",
    "\n",
    "        delta_S = (self.real_speed_R + self.real_speed_L) / 2               # Forward speed\n",
    "        self.v_Theta = (self.real_speed_R - self.real_speed_L) * speed_ratio / (2*wheel_dist)   # Angular velocity \n",
    "\n",
    "        self.v_X = delta_S * np.cos(self.X_est[4][0]) * speed_ratio         # SPEED in X\n",
    "        self.v_Y = delta_S * np.sin(self.X_est[4][0]) * speed_ratio         # SPEED in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "In summary, the Kalman Filter seemed to be the best option and the fact that we linearize the system before, using the odometry, we do not need to use the Exteded version even with a non-linear system. \n",
    "\n",
    "We calculated the different parameters for our covariance matrices $\\mathbf{R}$ and $\\mathbf{Q}$ based on measured samples directly taken with the camera and the Thymio's movement.The impact of the filter on the system allows smoother motion and to be more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[VIDEOS ? IMAGES ? LITTLE EXTRAS ?](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[Overall Project](#toc0_)\n",
    "\n",
    "The overall code can be run in the file main.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[Conclusion](#toc0_)\n",
    "\n",
    "Overall, through this project, we had the opportunity to get a better comprehension of the differents concepts seen in class. However, we have faced some difficulties in each sections allowing us to do an analysis of some advantages and disadvantages of our overall system.\n",
    "\n",
    "But in the end, we found very interesting to visualise how to create a project combining these concepts and have a deeper understanding of the purpose of the practical aspect of Mobile Robotics."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
